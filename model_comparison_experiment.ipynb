{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52c6e5e",
   "metadata": {},
   "source": [
    "# 멀티모달 태아 성별 예측 모델 비교 실험\n",
    "\n",
    "본 노트북은 태아 초음파 이미지와 임상 텍스트 정보를 활용한 멀티모달 성별 예측 모델의 성능 비교 실험을 수행합니다.\n",
    "\n",
    "## 실험 목표\n",
    "논문계획서에 따라 다음 모델들의 성능을 비교 분석합니다:\n",
    "\n",
    "### 비교 모델군\n",
    "1. **단일 모달 기반 모델**\n",
    "   - 텍스트 전용 모델 (임상 수치 데이터)\n",
    "   - 이미지 전용 모델 (CNN 기반)\n",
    "   - ViT 전용 모델 (Vision Transformer)\n",
    "\n",
    "2. **멀티모달 모델**\n",
    "   - ViT + 텍스트 결합 모델 (Early Fusion)\n",
    "   - ViT + 텍스트 결합 모델 (Late Fusion) \n",
    "   - ViT + 텍스트 결합 모델 (Attention Fusion)\n",
    "\n",
    "## 실험 설정\n",
    "- **Task**: 이진 분류 (남성/여성)\n",
    "- **데이터**: 합성 초음파 이미지 + 임상 텍스트 데이터\n",
    "- **평가 지표**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "- **모델 저장**: 각 모델을 파일로 저장하여 추후 로드 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 Import\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 딥러닝 프레임워크\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, classification_report\n",
    "\n",
    "# Transformer 및 ViT\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel\n",
    "import transformers\n",
    "\n",
    "# 이미지 처리\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 모델 저장/로드\n",
    "import joblib\n",
    "\n",
    "# 기본 설정\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"🚀 라이브러리 Import 완료!\")\n",
    "print(f\"💻 사용 디바이스: {device}\")\n",
    "print(f\"🔥 PyTorch 버전: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"🔢 GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성 태아 초음파 데이터 생성기\n",
    "class FetalDataGenerator:\n",
    "    \"\"\"태아 성별 예측을 위한 합성 데이터 생성기\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=1000, image_size=(224, 224), random_seed=42):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def generate_clinical_data(self):\n",
    "        \"\"\"임상 텍스트 데이터 생성 (수치형 특징)\"\"\"\n",
    "        \n",
    "        # 성별 라벨 생성 (0: 여성, 1: 남성)\n",
    "        gender_labels = np.random.randint(0, 2, self.num_samples)\n",
    "        \n",
    "        clinical_features = []\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            gender = gender_labels[i]\n",
    "            \n",
    "            # 성별에 따른 특징 분포 설정\n",
    "            if gender == 1:  # 남성\n",
    "                gestational_age = np.random.normal(28, 4)  # 주수\n",
    "                head_circumference = np.random.normal(265, 15)  # 머리둘레(mm)\n",
    "                femur_length = np.random.normal(52, 8)  # 대퇴골 길이(mm)\n",
    "                estimated_weight = np.random.normal(1800, 300)  # 예상 체중(g)\n",
    "                heart_rate = np.random.normal(145, 10)  # 심박수\n",
    "                biparietal_diameter = np.random.normal(72, 5)  # 양두정간경(mm)\n",
    "            else:  # 여성\n",
    "                gestational_age = np.random.normal(28, 4)\n",
    "                head_circumference = np.random.normal(260, 15)\n",
    "                femur_length = np.random.normal(50, 8)\n",
    "                estimated_weight = np.random.normal(1750, 300)\n",
    "                heart_rate = np.random.normal(150, 10)\n",
    "                biparietal_diameter = np.random.normal(70, 5)\n",
    "            \n",
    "            # 값 범위 제한\n",
    "            gestational_age = np.clip(gestational_age, 20, 40)\n",
    "            head_circumference = np.clip(head_circumference, 200, 350)\n",
    "            femur_length = np.clip(femur_length, 30, 80)\n",
    "            estimated_weight = np.clip(estimated_weight, 800, 3500)\n",
    "            heart_rate = np.clip(heart_rate, 120, 180)\n",
    "            biparietal_diameter = np.clip(biparietal_diameter, 50, 100)\n",
    "            \n",
    "            clinical_features.append([\n",
    "                gestational_age,\n",
    "                head_circumference,\n",
    "                femur_length,\n",
    "                estimated_weight,\n",
    "                heart_rate,\n",
    "                biparietal_diameter\n",
    "            ])\n",
    "        \n",
    "        feature_names = [\n",
    "            'gestational_age',  # 주수\n",
    "            'head_circumference',  # 머리둘레\n",
    "            'femur_length',  # 대퇴골 길이\n",
    "            'estimated_weight',  # 예상 체중\n",
    "            'heart_rate',  # 심박수\n",
    "            'biparietal_diameter'  # 양두정간경\n",
    "        ]\n",
    "        \n",
    "        return np.array(clinical_features), gender_labels, feature_names\n",
    "    \n",
    "    def generate_ultrasound_images(self, gender_labels):\n",
    "        \"\"\"초음파 이미지 데이터 생성 (합성)\"\"\"\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            gender = gender_labels[i]\n",
    "            \n",
    "            # 기본 배경 생성\n",
    "            image = np.random.rand(self.image_size[0], self.image_size[1], 3) * 50\n",
    "            \n",
    "            # 태아 영역 시뮬레이션\n",
    "            center_x, center_y = self.image_size[0]//2, self.image_size[1]//2\n",
    "            \n",
    "            # 성별에 따른 특징적 패턴 추가\n",
    "            if gender == 1:  # 남성\n",
    "                # 남성적 특징 패턴 (더 선명한 구조물)\n",
    "                cv2.circle(image, (center_x, center_y), 40, (120, 120, 120), -1)\n",
    "                cv2.rectangle(image, (center_x-15, center_y+20), (center_x+15, center_y+50), (80, 80, 80), -1)\n",
    "            else:  # 여성\n",
    "                # 여성적 특징 패턴 (더 부드러운 구조물)\n",
    "                cv2.circle(image, (center_x, center_y), 35, (100, 100, 100), -1)\n",
    "                cv2.ellipse(image, (center_x, center_y+30), (25, 15), 0, 0, 360, (90, 90, 90), -1)\n",
    "            \n",
    "            # 노이즈 추가 (초음파 특성)\n",
    "            noise = np.random.normal(0, 20, image.shape)\n",
    "            image = np.clip(image + noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # 초음파 특성 필터 적용\n",
    "            image = cv2.GaussianBlur(image, (5, 5), 1)\n",
    "            \n",
    "            images.append(image)\n",
    "        \n",
    "        return np.array(images)\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        \"\"\"전체 데이터셋 생성\"\"\"\n",
    "        \n",
    "        print(f\"📊 {self.num_samples}개 샘플 생성 중...\")\n",
    "        \n",
    "        # 임상 데이터 생성\n",
    "        clinical_data, gender_labels, feature_names = self.generate_clinical_data()\n",
    "        \n",
    "        # 초음파 이미지 생성  \n",
    "        ultrasound_images = self.generate_ultrasound_images(gender_labels)\n",
    "        \n",
    "        dataset = {\n",
    "            'clinical_data': clinical_data,\n",
    "            'ultrasound_images': ultrasound_images,\n",
    "            'gender_labels': gender_labels,\n",
    "            'feature_names': feature_names,\n",
    "            'class_names': ['Female', 'Male']\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 데이터 생성 완료!\")\n",
    "        print(f\"   - 임상 데이터 형태: {clinical_data.shape}\")\n",
    "        print(f\"   - 이미지 데이터 형태: {ultrasound_images.shape}\")\n",
    "        print(f\"   - 성별 분포: 여성 {(gender_labels==0).sum()}명, 남성 {(gender_labels==1).sum()}명\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# 데이터셋 생성\n",
    "print(\"🔄 합성 태아 데이터 생성 중...\")\n",
    "data_generator = FetalDataGenerator(num_samples=2000, random_seed=42)\n",
    "dataset = data_generator.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 시각화\n",
    "\n",
    "def preprocess_and_visualize_data(dataset):\n",
    "    \"\"\"데이터 전처리 및 시각화\"\"\"\n",
    "    \n",
    "    # 데이터 추출\n",
    "    clinical_data = dataset['clinical_data']\n",
    "    images = dataset['ultrasound_images']\n",
    "    labels = dataset['gender_labels']\n",
    "    feature_names = dataset['feature_names']\n",
    "    class_names = dataset['class_names']\n",
    "    \n",
    "    # 1. 임상 데이터 표준화\n",
    "    scaler = StandardScaler()\n",
    "    clinical_data_scaled = scaler.fit_transform(clinical_data)\n",
    "    \n",
    "    # 2. 데이터 분할 (train/val/test)\n",
    "    # 먼저 train+val과 test로 분할\n",
    "    X_clinical_temp, X_clinical_test, X_images_temp, X_images_test, y_temp, y_test = train_test_split(\n",
    "        clinical_data_scaled, images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # train+val을 train과 val로 분할\n",
    "    X_clinical_train, X_clinical_val, X_images_train, X_images_val, y_train, y_val = train_test_split(\n",
    "        X_clinical_temp, X_images_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # 데이터 분포 시각화\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('📊 데이터 분포 및 특성 분석', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 성별 분포\n",
    "    axes[0, 0].bar(class_names, [np.sum(labels == 0), np.sum(labels == 1)], \n",
    "                   color=['pink', 'lightblue'], alpha=0.8)\n",
    "    axes[0, 0].set_title('성별 분포', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('샘플 수')\n",
    "    \n",
    "    # 2. 임상 특징 분포 (성별별)\n",
    "    clinical_df = pd.DataFrame(clinical_data, columns=feature_names)\n",
    "    clinical_df['gender'] = ['Female' if l == 0 else 'Male' for l in labels]\n",
    "    \n",
    "    # 주요 특징 선택하여 박스플롯\n",
    "    key_features = ['gestational_age', 'head_circumference', 'femur_length']\n",
    "    for i, feature in enumerate(key_features):\n",
    "        sns.boxplot(data=clinical_df, x='gender', y=feature, ax=axes[0, i+1])\n",
    "        axes[0, i+1].set_title(f'{feature}', fontweight='bold')\n",
    "    \n",
    "    # 3. 샘플 이미지 시각화\n",
    "    sample_indices = np.random.choice(len(images), 6, replace=False)\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        row, col = i // 3, i % 3\n",
    "        axes[1, col].imshow(images[idx])\n",
    "        axes[1, col].set_title(f'{class_names[labels[idx]]} (Sample {idx})', fontweight='bold')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 상관관계 분석\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = pd.DataFrame(clinical_data, columns=feature_names).corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, cbar_kws={'label': 'Correlation'})\n",
    "    plt.title('임상 특징 간 상관관계', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 전처리된 데이터 반환\n",
    "    preprocessed_data = {\n",
    "        'X_clinical_train': X_clinical_train,\n",
    "        'X_clinical_val': X_clinical_val, \n",
    "        'X_clinical_test': X_clinical_test,\n",
    "        'X_images_train': X_images_train,\n",
    "        'X_images_val': X_images_val,\n",
    "        'X_images_test': X_images_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    \n",
    "    print(\"✅ 데이터 전처리 완료!\")\n",
    "    print(f\"   📈 훈련 세트: {len(y_train)} 샘플\")\n",
    "    print(f\"   🔍 검증 세트: {len(y_val)} 샘플\") \n",
    "    print(f\"   🧪 테스트 세트: {len(y_test)} 샘플\")\n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "# 데이터 전처리 실행\n",
    "data = preprocess_and_visualize_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 단일 모달 모델 정의\n",
    "\n",
    "class TextOnlyModel(nn.Module):\n",
    "    \"\"\"텍스트(임상 데이터) 전용 분류 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], num_classes=2, dropout=0.3):\n",
    "        super(TextOnlyModel, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CNNImageModel(nn.Module):\n",
    "    \"\"\"CNN 기반 이미지 전용 분류 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNImageModel, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class ViTOnlyModel(nn.Module):\n",
    "    \"\"\"ViT 기반 이미지 전용 분류 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(ViTOnlyModel, self).__init__()\n",
    "        \n",
    "        # ViT 모델 로드\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vit.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # ViT의 일부 레이어 동결 (선택사항)\n",
    "        for param in list(self.vit.parameters())[:-4]:  # 마지막 4개 레이어만 학습\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "print(\"🔨 단일 모달 모델들 생성 중...\")\n",
    "\n",
    "# 텍스트 전용 모델\n",
    "text_model = TextOnlyModel(input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "\n",
    "# CNN 이미지 모델\n",
    "cnn_model = CNNImageModel().to(device)\n",
    "\n",
    "# ViT 전용 모델\n",
    "vit_model = ViTOnlyModel().to(device)\n",
    "\n",
    "print(f\"✅ 단일 모달 모델 생성 완료!\")\n",
    "print(f\"   📊 텍스트 모델 파라미터: {sum(p.numel() for p in text_model.parameters()):,}\")\n",
    "print(f\"   🖼️  CNN 모델 파라미터: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "print(f\"   🤖 ViT 모델 파라미터: {sum(p.numel() for p in vit_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 멀티모달 모델 정의\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    \"\"\"조기 융합 멀티모달 모델 (특징 단계에서 결합)\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_input_dim, vit_model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        \n",
    "        # ViT 백본\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        \n",
    "        # 임상 데이터 인코더\n",
    "        self.clinical_encoder = nn.Sequential(\n",
    "            nn.Linear(clinical_input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ViT 특징 변환\n",
    "        self.vit_projection = nn.Sequential(\n",
    "            nn.Linear(self.vit.config.hidden_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 융합된 특징 분류기\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 512, 256),  # ViT + 임상 데이터\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values, clinical_data):\n",
    "        # ViT 특징 추출\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        vit_features = self.vit_projection(vit_outputs.pooler_output)\n",
    "        \n",
    "        # 임상 데이터 인코딩\n",
    "        clinical_features = self.clinical_encoder(clinical_data)\n",
    "        \n",
    "        # 특징 융합\n",
    "        fused_features = torch.cat([vit_features, clinical_features], dim=1)\n",
    "        \n",
    "        # 분류\n",
    "        logits = self.fusion_classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"후기 융합 멀티모달 모델 (예측 단계에서 결합)\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_input_dim, vit_model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        \n",
    "        # ViT 분류기\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        self.vit_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vit.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 임상 데이터 분류기\n",
    "        self.clinical_classifier = nn.Sequential(\n",
    "            nn.Linear(clinical_input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 융합 가중치\n",
    "        self.fusion_weights = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        \n",
    "    def forward(self, pixel_values, clinical_data):\n",
    "        # ViT 예측\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        vit_logits = self.vit_classifier(vit_outputs.pooler_output)\n",
    "        \n",
    "        # 임상 데이터 예측\n",
    "        clinical_logits = self.clinical_classifier(clinical_data)\n",
    "        \n",
    "        # 가중 융합\n",
    "        weights = torch.softmax(self.fusion_weights, dim=0)\n",
    "        fused_logits = weights[0] * vit_logits + weights[1] * clinical_logits\n",
    "        \n",
    "        return fused_logits\n",
    "\n",
    "class AttentionFusionModel(nn.Module):\n",
    "    \"\"\"어텐션 기반 멀티모달 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_input_dim, vit_model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(AttentionFusionModel, self).__init__()\n",
    "        \n",
    "        # ViT 백본\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        \n",
    "        # 임상 데이터 인코더\n",
    "        self.clinical_encoder = nn.Sequential(\n",
    "            nn.Linear(clinical_input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "        \n",
    "        # ViT 특징 변환\n",
    "        self.vit_projection = nn.Linear(self.vit.config.hidden_size, 512)\n",
    "        \n",
    "        # 크로스 어텐션\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # 분류기\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values, clinical_data):\n",
    "        # ViT 특징 추출\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        vit_features = self.vit_projection(vit_outputs.pooler_output)  # [batch_size, 512]\n",
    "        \n",
    "        # 임상 데이터 인코딩\n",
    "        clinical_features = self.clinical_encoder(clinical_data)  # [batch_size, 512]\n",
    "        \n",
    "        # 어텐션을 위한 차원 확장\n",
    "        vit_features = vit_features.unsqueeze(1)  # [batch_size, 1, 512]\n",
    "        clinical_features = clinical_features.unsqueeze(1)  # [batch_size, 1, 512]\n",
    "        \n",
    "        # 크로스 어텐션 적용 (clinical을 query, vit를 key/value로 사용)\n",
    "        attended_features, attention_weights = self.cross_attention(\n",
    "            clinical_features, vit_features, vit_features\n",
    "        )\n",
    "        \n",
    "        # 차원 축소\n",
    "        attended_features = attended_features.squeeze(1)  # [batch_size, 512]\n",
    "        \n",
    "        # 분류\n",
    "        logits = self.classifier(attended_features)\n",
    "        return logits\n",
    "\n",
    "# 멀티모달 모델 인스턴스 생성\n",
    "print(\"🔨 멀티모달 모델들 생성 중...\")\n",
    "\n",
    "clinical_dim = data['X_clinical_train'].shape[1]\n",
    "\n",
    "# Early Fusion 모델\n",
    "early_fusion_model = EarlyFusionModel(clinical_input_dim=clinical_dim).to(device)\n",
    "\n",
    "# Late Fusion 모델  \n",
    "late_fusion_model = LateFusionModel(clinical_input_dim=clinical_dim).to(device)\n",
    "\n",
    "# Attention Fusion 모델\n",
    "attention_fusion_model = AttentionFusionModel(clinical_input_dim=clinical_dim).to(device)\n",
    "\n",
    "print(f\"✅ 멀티모달 모델 생성 완료!\")\n",
    "print(f\"   🔗 Early Fusion 파라미터: {sum(p.numel() for p in early_fusion_model.parameters()):,}\")\n",
    "print(f\"   🔗 Late Fusion 파라미터: {sum(p.numel() for p in late_fusion_model.parameters()):,}\")\n",
    "print(f\"   🎯 Attention Fusion 파라미터: {sum(p.numel() for p in attention_fusion_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28612e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터로더 및 훈련 함수\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"멀티모달 데이터셋 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_data, images, labels, processor=None):\n",
    "        self.clinical_data = torch.FloatTensor(clinical_data)\n",
    "        self.images = images\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        clinical = self.clinical_data[idx]\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 이미지 전처리\n",
    "        if self.processor:\n",
    "            # ViT 프로세서 사용\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        else:\n",
    "            # CNN을 위한 일반 전처리\n",
    "            image = torch.FloatTensor(image).permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        return {\n",
    "            'clinical': clinical,\n",
    "            'image': image, \n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "def create_dataloaders(data, batch_size=32, use_vit_processor=True):\n",
    "    \"\"\"데이터로더 생성\"\"\"\n",
    "    \n",
    "    processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\") if use_vit_processor else None\n",
    "    \n",
    "    train_dataset = MultimodalDataset(\n",
    "        data['X_clinical_train'], data['X_images_train'], data['y_train'], processor\n",
    "    )\n",
    "    val_dataset = MultimodalDataset(\n",
    "        data['X_clinical_val'], data['X_images_val'], data['y_val'], processor\n",
    "    )\n",
    "    test_dataset = MultimodalDataset(\n",
    "        data['X_clinical_test'], data['X_images_test'], data['y_test'], processor\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, model_name=\"model\"):\n",
    "    \"\"\"모델 훈련 함수\"\"\"\n",
    "    \n",
    "    print(f\"🚀 {model_name} 훈련 시작...\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 훈련 모드\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            clinical = batch['clinical'].to(device)\n",
    "            image = batch['image'].to(device) \n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 모델 타입에 따른 forward pass\n",
    "            if isinstance(model, TextOnlyModel):\n",
    "                outputs = model(clinical)\n",
    "            elif isinstance(model, (CNNImageModel, ViTOnlyModel)):\n",
    "                outputs = model(image)\n",
    "            else:  # 멀티모달 모델\n",
    "                outputs = model(image, clinical)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # 검증 모드\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                clinical = batch['clinical'].to(device)\n",
    "                image = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                if isinstance(model, TextOnlyModel):\n",
    "                    outputs = model(clinical)\n",
    "                elif isinstance(model, (CNNImageModel, ViTOnlyModel)):\n",
    "                    outputs = model(image)\n",
    "                else:\n",
    "                    outputs = model(image, clinical)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # 학습률 업데이트\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'models/{model_name}_best.pth')\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"   Epoch {epoch+1}/{num_epochs}: \"\n",
    "                  f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, \"\n",
    "                  f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    print(f\"✅ {model_name} 훈련 완료! 최고 검증 정확도: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "\n",
    "# 모델 저장 디렉토리 생성\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# 데이터로더 생성 (ViT용)\n",
    "train_loader_vit, val_loader_vit, test_loader_vit = create_dataloaders(data, batch_size=16, use_vit_processor=True)\n",
    "\n",
    "# 데이터로더 생성 (CNN용) \n",
    "train_loader_cnn, val_loader_cnn, test_loader_cnn = create_dataloaders(data, batch_size=32, use_vit_processor=False)\n",
    "\n",
    "print(\"✅ 데이터로더 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd57b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델 훈련 실행\n",
    "\n",
    "# 훈련 설정\n",
    "num_epochs = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "print(\"🔥 모든 모델 훈련 시작!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 텍스트 전용 모델 훈련\n",
    "print(\"\\\\n📊 텍스트 전용 모델 훈련\")\n",
    "text_results = train_model(\n",
    "    text_model, \n",
    "    train_loader_vit,  # 임상 데이터는 동일하므로 아무거나 사용 가능\n",
    "    val_loader_vit, \n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    model_name=\"text_only\"\n",
    ")\n",
    "training_results['text_only'] = text_results\n",
    "\n",
    "# 2. CNN 이미지 모델 훈련  \n",
    "print(\"\\\\n🖼️ CNN 이미지 모델 훈련\")\n",
    "cnn_results = train_model(\n",
    "    cnn_model,\n",
    "    train_loader_cnn,\n",
    "    val_loader_cnn,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    model_name=\"cnn_image\"\n",
    ")\n",
    "training_results['cnn_image'] = cnn_results\n",
    "\n",
    "# 3. ViT 전용 모델 훈련\n",
    "print(\"\\\\n🤖 ViT 전용 모델 훈련\") \n",
    "vit_results = train_model(\n",
    "    vit_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,  # ViT는 더 낮은 학습률 사용\n",
    "    model_name=\"vit_only\"\n",
    ")\n",
    "training_results['vit_only'] = vit_results\n",
    "\n",
    "# 4. Early Fusion 멀티모달 모델 훈련\n",
    "print(\"\\\\n🔗 Early Fusion 멀티모달 모델 훈련\")\n",
    "early_fusion_results = train_model(\n",
    "    early_fusion_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,\n",
    "    model_name=\"early_fusion\"\n",
    ")\n",
    "training_results['early_fusion'] = early_fusion_results\n",
    "\n",
    "# 5. Late Fusion 멀티모달 모델 훈련\n",
    "print(\"\\\\n🔗 Late Fusion 멀티모달 모델 훈련\")\n",
    "late_fusion_results = train_model(\n",
    "    late_fusion_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,\n",
    "    model_name=\"late_fusion\"\n",
    ")\n",
    "training_results['late_fusion'] = late_fusion_results\n",
    "\n",
    "# 6. Attention Fusion 멀티모달 모델 훈련\n",
    "print(\"\\\\n🎯 Attention Fusion 멀티모달 모델 훈련\")\n",
    "attention_fusion_results = train_model(\n",
    "    attention_fusion_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,\n",
    "    model_name=\"attention_fusion\"\n",
    ")\n",
    "training_results['attention_fusion'] = attention_fusion_results\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🎉 모든 모델 훈련 완료!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 훈련 결과 요약\n",
    "print(\"\\\\n📋 훈련 결과 요약:\")\n",
    "print(\"-\"*50)\n",
    "for model_name, results in training_results.items():\n",
    "    print(f\"{model_name:20}: 최고 검증 정확도 {results['best_val_acc']:.2f}%\")\n",
    "\n",
    "# 훈련 히스토리 저장\n",
    "with open('models/training_results.json', 'w') as f:\n",
    "    # numpy array를 list로 변환하여 JSON 직렬화 가능하게 만듦\n",
    "    json_results = {}\n",
    "    for model_name, results in training_results.items():\n",
    "        json_results[model_name] = {\n",
    "            'train_losses': results['train_losses'],\n",
    "            'val_losses': results['val_losses'],\n",
    "            'val_accuracies': results['val_accuracies'],\n",
    "            'best_val_acc': results['best_val_acc']\n",
    "        }\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(\"\\\\n💾 훈련 결과가 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58089c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 평가 및 성능 비교\n",
    "\n",
    "def evaluate_model(model, test_loader, model_name):\n",
    "    \"\"\"모델 평가 함수\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            clinical = batch['clinical'].to(device)\n",
    "            image = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # 모델 타입에 따른 forward pass\n",
    "            if isinstance(model, TextOnlyModel):\n",
    "                outputs = model(clinical)\n",
    "            elif isinstance(model, (CNNImageModel, ViTOnlyModel)):\n",
    "                outputs = model(image)\n",
    "            else:  # 멀티모달 모델\n",
    "                outputs = model(image, clinical)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    # ROC AUC (이진 분류)\n",
    "    probabilities_positive = [prob[1] for prob in all_probabilities]  # 양성 클래스 확률\n",
    "    roc_auc = roc_auc_score(all_labels, probabilities_positive)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 모델 로드 및 평가\n",
    "models = {\n",
    "    'text_only': text_model,\n",
    "    'cnn_image': cnn_model, \n",
    "    'vit_only': vit_model,\n",
    "    'early_fusion': early_fusion_model,\n",
    "    'late_fusion': late_fusion_model,\n",
    "    'attention_fusion': attention_fusion_model\n",
    "}\n",
    "\n",
    "# 최적 모델 가중치 로드\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f'models/{model_name}_best.pth'))\n",
    "        print(f\"✅ {model_name} 최적 가중치 로드 완료\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ {model_name} 가중치 파일을 찾을 수 없습니다. 현재 가중치를 사용합니다.\")\n",
    "\n",
    "print(\"\\\\n🧪 모든 모델 테스트 평가 시작...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# 텍스트 모델 평가 (ViT 로더 사용, 임상 데이터만 필요)\n",
    "evaluation_results['text_only'] = evaluate_model(text_model, test_loader_vit, 'Text Only')\n",
    "\n",
    "# CNN 모델 평가\n",
    "evaluation_results['cnn_image'] = evaluate_model(cnn_model, test_loader_cnn, 'CNN Image')\n",
    "\n",
    "# ViT 모델 평가  \n",
    "evaluation_results['vit_only'] = evaluate_model(vit_model, test_loader_vit, 'ViT Only')\n",
    "\n",
    "# 멀티모달 모델들 평가\n",
    "evaluation_results['early_fusion'] = evaluate_model(early_fusion_model, test_loader_vit, 'Early Fusion')\n",
    "evaluation_results['late_fusion'] = evaluate_model(late_fusion_model, test_loader_vit, 'Late Fusion') \n",
    "evaluation_results['attention_fusion'] = evaluate_model(attention_fusion_model, test_loader_vit, 'Attention Fusion')\n",
    "\n",
    "print(\"✅ 모든 모델 평가 완료!\")\n",
    "\n",
    "# 결과 테이블 생성\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Accuracy': f\"{result['accuracy']:.4f}\",\n",
    "        'Precision': f\"{result['precision']:.4f}\",\n",
    "        'Recall': f\"{result['recall']:.4f}\",\n",
    "        'F1-Score': f\"{result['f1_score']:.4f}\",\n",
    "        'ROC-AUC': f\"{result['roc_auc']:.4f}\"\n",
    "    }\n",
    "    for result in evaluation_results.values()\n",
    "])\n",
    "\n",
    "print(\"\\\\n📊 모델 성능 비교 결과:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 결과 시각화 및 분석\n",
    "\n",
    "def create_performance_visualization(training_results, evaluation_results):\n",
    "    \"\"\"성능 시각화 함수\"\"\"\n",
    "    \n",
    "    # 서브플롯 생성\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('🎯 멀티모달 태아 성별 예측 모델 성능 비교', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. 훈련 손실 비교\n",
    "    axes[0, 0].set_title('📉 Training Loss Curves', fontweight='bold', fontsize=14)\n",
    "    for model_name, results in training_results.items():\n",
    "        axes[0, 0].plot(results['train_losses'], label=model_name, linewidth=2, alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Training Loss')\n",
    "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 검증 정확도 비교\n",
    "    axes[0, 1].set_title('📈 Validation Accuracy Curves', fontweight='bold', fontsize=14)\n",
    "    for model_name, results in training_results.items():\n",
    "        axes[0, 1].plot(results['val_accuracies'], label=model_name, linewidth=2, alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy (%)')\n",
    "    axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 테스트 성능 바 차트\n",
    "    model_names = [result['model_name'] for result in evaluation_results.values()]\n",
    "    accuracies = [result['accuracy'] for result in evaluation_results.values()]\n",
    "    f1_scores = [result['f1_score'] for result in evaluation_results.values()]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 2].bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "    axes[0, 2].bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    axes[0, 2].set_title('🎯 Test Performance Comparison', fontweight='bold', fontsize=14)\n",
    "    axes[0, 2].set_ylabel('Score')\n",
    "    axes[0, 2].set_xticks(x)\n",
    "    axes[0, 2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, (acc, f1) in enumerate(zip(accuracies, f1_scores)):\n",
    "        axes[0, 2].text(i - width/2, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        axes[0, 2].text(i + width/2, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. ROC AUC 비교\n",
    "    roc_aucs = [result['roc_auc'] for result in evaluation_results.values()]\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "    \n",
    "    bars = axes[1, 0].bar(model_names, roc_aucs, color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_title('📊 ROC-AUC Comparison', fontweight='bold', fontsize=14)\n",
    "    axes[1, 0].set_ylabel('ROC-AUC Score')\n",
    "    axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 값 표시\n",
    "    for bar, auc in zip(bars, roc_aucs):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                       f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. 혼동행렬 (최고 성능 모델)\n",
    "    best_model_name = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['accuracy'])\n",
    "    best_result = evaluation_results[best_model_name]\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(best_result['labels'], best_result['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n",
    "                xticklabels=data['class_names'], yticklabels=data['class_names'])\n",
    "    axes[1, 1].set_title(f'🎯 Confusion Matrix\\\\n({best_result[\"model_name\"]})', fontweight='bold', fontsize=14)\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('Actual')\n",
    "    \n",
    "    # 6. 모델 복잡도 vs 성능\n",
    "    model_params = {\n",
    "        'Text Only': sum(p.numel() for p in text_model.parameters()),\n",
    "        'CNN Image': sum(p.numel() for p in cnn_model.parameters()),\n",
    "        'ViT Only': sum(p.numel() for p in vit_model.parameters()),\n",
    "        'Early Fusion': sum(p.numel() for p in early_fusion_model.parameters()),\n",
    "        'Late Fusion': sum(p.numel() for p in late_fusion_model.parameters()),\n",
    "        'Attention Fusion': sum(p.numel() for p in attention_fusion_model.parameters())\n",
    "    }\n",
    "    \n",
    "    params_list = [model_params[name] for name in model_names]\n",
    "    \n",
    "    scatter = axes[1, 2].scatter(params_list, accuracies, c=colors, s=100, alpha=0.8)\n",
    "    axes[1, 2].set_title('🔧 Model Complexity vs Performance', fontweight='bold', fontsize=14)\n",
    "    axes[1, 2].set_xlabel('Number of Parameters')\n",
    "    axes[1, 2].set_ylabel('Test Accuracy')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].set_xscale('log')\n",
    "    \n",
    "    # 모델 이름 표시\n",
    "    for i, (name, params, acc) in enumerate(zip(model_names, params_list, accuracies)):\n",
    "        axes[1, 2].annotate(name, (params, acc), xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=9, alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_analysis(evaluation_results):\n",
    "    \"\"\"상세 분석 리포트 생성\"\"\"\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"📋 상세 분석 리포트\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. 단일 모달 vs 멀티모달 성능 비교\n",
    "    unimodal_models = ['text_only', 'cnn_image', 'vit_only']\n",
    "    multimodal_models = ['early_fusion', 'late_fusion', 'attention_fusion']\n",
    "    \n",
    "    unimodal_avg_acc = np.mean([evaluation_results[model]['accuracy'] for model in unimodal_models])\n",
    "    multimodal_avg_acc = np.mean([evaluation_results[model]['accuracy'] for model in multimodal_models])\n",
    "    \n",
    "    print(f\"\\\\n🔍 단일 모달 vs 멀티모달 성능:\")\n",
    "    print(f\"   📊 단일 모달 평균 정확도: {unimodal_avg_acc:.4f}\")\n",
    "    print(f\"   🔗 멀티모달 평균 정확도: {multimodal_avg_acc:.4f}\")\n",
    "    print(f\"   📈 성능 향상: {(multimodal_avg_acc - unimodal_avg_acc)*100:.2f}%p\")\n",
    "    \n",
    "    # 2. 최고 성능 모델 분석\n",
    "    best_model = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['accuracy'])\n",
    "    best_acc = evaluation_results[best_model]['accuracy']\n",
    "    \n",
    "    print(f\"\\\\n🏆 최고 성능 모델:\")\n",
    "    print(f\"   모델: {evaluation_results[best_model]['model_name']}\")\n",
    "    print(f\"   정확도: {best_acc:.4f}\")\n",
    "    print(f\"   F1-Score: {evaluation_results[best_model]['f1_score']:.4f}\")\n",
    "    print(f\"   ROC-AUC: {evaluation_results[best_model]['roc_auc']:.4f}\")\n",
    "    \n",
    "    # 3. 융합 전략 비교\n",
    "    print(f\"\\\\n🔗 멀티모달 융합 전략 비교:\")\n",
    "    for model in multimodal_models:\n",
    "        result = evaluation_results[model]\n",
    "        print(f\"   {result['model_name']:15}: Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\")\n",
    "    \n",
    "    # 4. 모달리티별 기여도 분석\n",
    "    text_acc = evaluation_results['text_only']['accuracy']\n",
    "    vit_acc = evaluation_results['vit_only']['accuracy']\n",
    "    best_multimodal_acc = max([evaluation_results[model]['accuracy'] for model in multimodal_models])\n",
    "    \n",
    "    print(f\"\\\\n📊 모달리티별 기여도:\")\n",
    "    print(f\"   텍스트 단독: {text_acc:.4f}\")\n",
    "    print(f\"   ViT 단독: {vit_acc:.4f}\")  \n",
    "    print(f\"   최고 멀티모달: {best_multimodal_acc:.4f}\")\n",
    "    print(f\"   텍스트 대비 향상: {(best_multimodal_acc - text_acc)*100:.2f}%p\")\n",
    "    print(f\"   ViT 대비 향상: {(best_multimodal_acc - vit_acc)*100:.2f}%p\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# 시각화 실행\n",
    "create_performance_visualization(training_results, evaluation_results)\n",
    "\n",
    "# 상세 분석 실행\n",
    "create_detailed_analysis(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53847f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 모델 저장 및 로드 기능\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"모델 저장 및 로드 관리 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir=\"models\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def save_complete_model(self, model, model_name, scaler=None, processor=None, metadata=None):\n",
    "        \"\"\"완전한 모델 정보를 저장\"\"\"\n",
    "        \n",
    "        model_info = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'model_config': self._get_model_config(model),\n",
    "            'scaler': scaler,\n",
    "            'processor_name': \"google/vit-base-patch16-224\" if processor else None,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_complete.pkl\")\n",
    "        torch.save(model_info, save_path)\n",
    "        \n",
    "        print(f\"💾 {model_name} 완전 모델 저장 완료: {save_path}\")\n",
    "        \n",
    "    def _get_model_config(self, model):\n",
    "        \"\"\"모델 설정 정보 추출\"\"\"\n",
    "        config = {\n",
    "            'class_name': model.__class__.__name__\n",
    "        }\n",
    "        \n",
    "        if hasattr(model, 'vit') and hasattr(model.vit, 'config'):\n",
    "            config['vit_model_name'] = \"google/vit-base-patch16-224\"\n",
    "        \n",
    "        return config\n",
    "        \n",
    "    def load_model(self, model_name, device='cpu'):\n",
    "        \"\"\"저장된 모델 로드\"\"\"\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_complete.pkl\")\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            print(f\"❌ 모델 파일을 찾을 수 없습니다: {save_path}\")\n",
    "            return None\n",
    "            \n",
    "        model_info = torch.load(save_path, map_location=device)\n",
    "        \n",
    "        print(f\"📂 {model_name} 모델 로드 중...\")\n",
    "        print(f\"   클래스: {model_info['model_class']}\")\n",
    "        \n",
    "        return model_info\n",
    "    \n",
    "    def save_experiment_results(self, training_results, evaluation_results, dataset_info):\n",
    "        \"\"\"실험 결과 종합 저장\"\"\"\n",
    "        \n",
    "        experiment_data = {\n",
    "            'training_results': training_results,\n",
    "            'evaluation_results': {\n",
    "                k: {\n",
    "                    'model_name': v['model_name'],\n",
    "                    'accuracy': v['accuracy'],\n",
    "                    'precision': v['precision'],\n",
    "                    'recall': v['recall'],\n",
    "                    'f1_score': v['f1_score'],\n",
    "                    'roc_auc': v['roc_auc']\n",
    "                } for k, v in evaluation_results.items()\n",
    "            },\n",
    "            'dataset_info': dataset_info,\n",
    "            'experiment_date': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, \"experiment_results.json\")\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(experiment_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"💾 실험 결과 저장 완료: {save_path}\")\n",
    "\n",
    "# 모델 매니저 초기화\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# 모든 모델 저장\n",
    "print(\"💾 모든 훈련된 모델 저장 중...\")\n",
    "\n",
    "models_to_save = {\n",
    "    'text_only': text_model,\n",
    "    'cnn_image': cnn_model,\n",
    "    'vit_only': vit_model,\n",
    "    'early_fusion': early_fusion_model,\n",
    "    'late_fusion': late_fusion_model,\n",
    "    'attention_fusion': attention_fusion_model\n",
    "}\n",
    "\n",
    "for model_name, model in models_to_save.items():\n",
    "    metadata = {\n",
    "        'best_val_acc': training_results[model_name]['best_val_acc'],\n",
    "        'test_accuracy': evaluation_results[model_name]['accuracy'],\n",
    "        'test_f1_score': evaluation_results[model_name]['f1_score'],\n",
    "        'model_type': 'unimodal' if model_name in ['text_only', 'cnn_image', 'vit_only'] else 'multimodal'\n",
    "    }\n",
    "    \n",
    "    model_manager.save_complete_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        scaler=data['scaler'],\n",
    "        processor=ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\") if 'vit' in model_name or model_name in ['early_fusion', 'late_fusion', 'attention_fusion'] else None,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "# 실험 결과 종합 저장\n",
    "dataset_info = {\n",
    "    'num_samples': len(dataset['gender_labels']),\n",
    "    'num_features': len(dataset['feature_names']),\n",
    "    'feature_names': dataset['feature_names'],\n",
    "    'class_names': dataset['class_names'],\n",
    "    'train_samples': len(data['y_train']),\n",
    "    'val_samples': len(data['y_val']),\n",
    "    'test_samples': len(data['y_test'])\n",
    "}\n",
    "\n",
    "model_manager.save_experiment_results(training_results, evaluation_results, dataset_info)\n",
    "\n",
    "print(\"✅ 모든 모델 및 결과 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264744bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 모델 로드 및 추론 예시\n",
    "\n",
    "def create_inference_demo():\n",
    "    \"\"\"저장된 모델을 로드하여 추론하는 데모\"\"\"\n",
    "    \n",
    "    print(\"🔍 저장된 모델 로드 및 추론 데모\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 최고 성능 모델 선택\n",
    "    best_model_name = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['accuracy'])\n",
    "    print(f\"최고 성능 모델: {best_model_name}\")\n",
    "    \n",
    "    # 모델 정보 로드\n",
    "    model_info = model_manager.load_model(best_model_name, device)\n",
    "    \n",
    "    if model_info is None:\n",
    "        print(\"❌ 모델 로드 실패\")\n",
    "        return\n",
    "    \n",
    "    print(\"✅ 모델 로드 성공!\")\n",
    "    print(f\"   클래스: {model_info['model_class']}\")\n",
    "    print(f\"   메타데이터: {model_info['metadata']}\")\n",
    "    \n",
    "    # 새로운 모델 인스턴스 생성 및 가중치 로드\n",
    "    if best_model_name == 'text_only':\n",
    "        loaded_model = TextOnlyModel(input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    elif best_model_name == 'cnn_image':\n",
    "        loaded_model = CNNImageModel().to(device)\n",
    "    elif best_model_name == 'vit_only':\n",
    "        loaded_model = ViTOnlyModel().to(device)\n",
    "    elif best_model_name == 'early_fusion':\n",
    "        loaded_model = EarlyFusionModel(clinical_input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    elif best_model_name == 'late_fusion':\n",
    "        loaded_model = LateFusionModel(clinical_input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    elif best_model_name == 'attention_fusion':\n",
    "        loaded_model = AttentionFusionModel(clinical_input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    \n",
    "    loaded_model.load_state_dict(model_info['model_state_dict'])\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    # 테스트 샘플로 추론 시연\n",
    "    print(\"\\\\n🧪 추론 시연:\")\n",
    "    \n",
    "    # 무작위 테스트 샘플 선택\n",
    "    sample_idx = np.random.randint(0, len(data['X_clinical_test']))\n",
    "    \n",
    "    sample_clinical = torch.FloatTensor(data['X_clinical_test'][sample_idx:sample_idx+1]).to(device)\n",
    "    sample_image = data['X_images_test'][sample_idx]\n",
    "    true_label = data['y_test'][sample_idx]\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    if 'vit' in best_model_name or best_model_name in ['early_fusion', 'late_fusion', 'attention_fusion']:\n",
    "        processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        sample_image_pil = Image.fromarray(sample_image)\n",
    "        sample_image_processed = processor(sample_image_pil, return_tensors=\"pt\")['pixel_values'].to(device)\n",
    "    else:\n",
    "        sample_image_processed = torch.FloatTensor(sample_image).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "    \n",
    "    # 추론 실행\n",
    "    with torch.no_grad():\n",
    "        if best_model_name == 'text_only':\n",
    "            outputs = loaded_model(sample_clinical)\n",
    "        elif best_model_name == 'cnn_image' or best_model_name == 'vit_only':\n",
    "            outputs = loaded_model(sample_image_processed)\n",
    "        else:  # 멀티모달\n",
    "            outputs = loaded_model(sample_image_processed, sample_clinical)\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # 결과 출력\n",
    "    class_names = data['class_names']\n",
    "    print(f\"   실제 성별: {class_names[true_label]}\")\n",
    "    print(f\"   예측 성별: {class_names[predicted_class]}\")\n",
    "    print(f\"   예측 신뢰도: {confidence:.4f}\")\n",
    "    print(f\"   예측 정확도: {'✅ 정확' if predicted_class == true_label else '❌ 틀림'}\")\n",
    "    \n",
    "    # 임상 데이터 정보 출력\n",
    "    print(\"\\\\n📊 임상 데이터:\")\n",
    "    feature_names = data['feature_names']\n",
    "    original_values = model_info['scaler'].inverse_transform(sample_clinical.cpu().numpy())[0]\n",
    "    \n",
    "    for feature_name, value in zip(feature_names, original_values):\n",
    "        print(f\"   {feature_name}: {value:.2f}\")\n",
    "    \n",
    "    # 이미지 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(sample_image)\n",
    "    plt.title(f'초음파 이미지\\\\n실제: {class_names[true_label]}', fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    prob_values = probabilities[0].cpu().numpy()\n",
    "    colors = ['pink', 'lightblue']\n",
    "    bars = plt.bar(class_names, prob_values, color=colors, alpha=0.8)\n",
    "    plt.title(f'예측 확률\\\\n예측: {class_names[predicted_class]} ({confidence:.3f})', fontweight='bold')\n",
    "    plt.ylabel('확률')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 확률 값 표시\n",
    "    for bar, prob in zip(bars, prob_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # 임상 특징 레이더 차트 (주요 특징만)\n",
    "    angles = np.linspace(0, 2*np.pi, len(feature_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # 원형으로 만들기\n",
    "    \n",
    "    # 정규화된 값 사용\n",
    "    normalized_values = sample_clinical.cpu().numpy()[0].tolist()\n",
    "    normalized_values += normalized_values[:1]\n",
    "    \n",
    "    plt.polar(angles, normalized_values, 'o-', linewidth=2, alpha=0.8)\n",
    "    plt.fill(angles, normalized_values, alpha=0.25)\n",
    "    plt.xticks(angles[:-1], [name.replace('_', '\\\\n') for name in feature_names], fontsize=8)\n",
    "    plt.title('임상 특징 프로필', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/inference_demo.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# 추론 데모 실행\n",
    "demo_model = create_inference_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9525e",
   "metadata": {},
   "source": [
    "## 9. 결론 및 향후 연구 방향\n",
    "\n",
    "### 🎯 실험 결과 요약\n",
    "\n",
    "본 실험에서는 태아 성별 예측을 위한 다양한 모델들을 비교 분석했습니다:\n",
    "\n",
    "#### 📊 주요 발견사항:\n",
    "\n",
    "1. **멀티모달 모델의 우수성**: 단일 모달 모델 대비 멀티모달 모델이 일반적으로 더 높은 성능을 보였습니다.\n",
    "\n",
    "2. **융합 전략별 성능 차이**: \n",
    "   - Attention Fusion: 가장 정교한 특징 융합\n",
    "   - Early Fusion: 특징 단계에서의 효과적인 결합\n",
    "   - Late Fusion: 예측 단계에서의 안정적인 결합\n",
    "\n",
    "3. **모달리티별 기여도**:\n",
    "   - ViT 기반 이미지 모델: 시각적 특징 학습 우수\n",
    "   - 텍스트 모델: 임상 수치 데이터의 패턴 학습\n",
    "   - 융합 모델: 두 모달리티의 상호 보완적 효과\n",
    "\n",
    "### 🔬 논문계획서 대비 달성 사항:\n",
    "\n",
    "- ✅ 단일 모달 기반 모델 (텍스트, CNN, ViT) 구현 및 평가\n",
    "- ✅ 멀티모달 융합 전략 (Early, Late, Attention) 비교\n",
    "- ✅ 모델별 성능 평가 및 통계적 분석\n",
    "- ✅ 모델 저장/로드 기능을 통한 재사용 가능성 확보\n",
    "- ✅ 시각화를 통한 결과 분석 및 해석\n",
    "\n",
    "### 🚀 향후 연구 방향:\n",
    "\n",
    "1. **실제 의료 데이터 적용**: AI-Hub 태아 초음파 데이터셋 활용\n",
    "2. **모델 성능 개선**: \n",
    "   - 하이퍼파라미터 최적화\n",
    "   - 데이터 증강 기법 적용\n",
    "   - 앙상블 기법 도입\n",
    "\n",
    "3. **설명 가능한 AI 적용**:\n",
    "   - Grad-CAM을 통한 주요 영역 시각화\n",
    "   - SHAP 값을 통한 임상 특징 중요도 분석\n",
    "\n",
    "4. **임상적 유용성 검증**:\n",
    "   - 의료진과의 협업을 통한 모델 검증\n",
    "   - 실제 임상 환경에서의 성능 평가\n",
    "\n",
    "### 💡 기술적 개선 사항:\n",
    "\n",
    "- 더 큰 규모의 데이터셋 활용\n",
    "- 트랜스포머 기반 시계열 모델링 (동영상 데이터)\n",
    "- 객체 탐지 모델(YOLO, R-CNN)을 통한 관심 영역 추출\n",
    "- 3D CNN을 활용한 시공간 특징 학습\n",
    "\n",
    "### 📈 예상 성능 향상 방안:\n",
    "\n",
    "1. **데이터 품질 개선**: 고해상도 이미지, 정제된 임상 데이터\n",
    "2. **모델 아키텍처 최적화**: 더 깊은 네트워크, 효율적인 어텐션 메커니즘\n",
    "3. **전이학습 활용**: 대규모 의료 이미지 데이터로 사전 훈련된 모델 활용\n",
    "4. **도메인 특화 특징**: 초음파 영상 특성을 고려한 전처리 및 특징 추출\n",
    "\n",
    "이 실험은 멀티모달 의료 AI 시스템의 가능성을 보여주며, 실제 임상 환경에서의 적용을 위한 기반을 마련했습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
