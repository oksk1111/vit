{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52c6e5e",
   "metadata": {},
   "source": [
    "# ë©€í‹°ëª¨ë‹¬ íƒœì•„ ì„±ë³„ ì˜ˆì¸¡ ëª¨ë¸ ë¹„êµ ì‹¤í—˜\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ íƒœì•„ ì´ˆìŒíŒŒ ì´ë¯¸ì§€ì™€ ì„ìƒ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í™œìš©í•œ ë©€í‹°ëª¨ë‹¬ ì„±ë³„ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì‹¤í—˜ ëª©í‘œ\n",
    "ë…¼ë¬¸ê³„íšì„œì— ë”°ë¼ ë‹¤ìŒ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤:\n",
    "\n",
    "### ë¹„êµ ëª¨ë¸êµ°\n",
    "1. **ë‹¨ì¼ ëª¨ë‹¬ ê¸°ë°˜ ëª¨ë¸**\n",
    "   - í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ (ì„ìƒ ìˆ˜ì¹˜ ë°ì´í„°)\n",
    "   - ì´ë¯¸ì§€ ì „ìš© ëª¨ë¸ (CNN ê¸°ë°˜)\n",
    "   - ViT ì „ìš© ëª¨ë¸ (Vision Transformer)\n",
    "\n",
    "2. **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸**\n",
    "   - ViT + í…ìŠ¤íŠ¸ ê²°í•© ëª¨ë¸ (Early Fusion)\n",
    "   - ViT + í…ìŠ¤íŠ¸ ê²°í•© ëª¨ë¸ (Late Fusion) \n",
    "   - ViT + í…ìŠ¤íŠ¸ ê²°í•© ëª¨ë¸ (Attention Fusion)\n",
    "\n",
    "## ì‹¤í—˜ ì„¤ì •\n",
    "- **Task**: ì´ì§„ ë¶„ë¥˜ (ë‚¨ì„±/ì—¬ì„±)\n",
    "- **ë°ì´í„°**: í•©ì„± ì´ˆìŒíŒŒ ì´ë¯¸ì§€ + ì„ìƒ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "- **í‰ê°€ ì§€í‘œ**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "- **ëª¨ë¸ ì €ì¥**: ê° ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì¶”í›„ ë¡œë“œ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, classification_report\n",
    "\n",
    "# Transformer ë° ViT\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel\n",
    "import transformers\n",
    "\n",
    "# ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥/ë¡œë“œ\n",
    "import joblib\n",
    "\n",
    "# ê¸°ë³¸ ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"ğŸš€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ì™„ë£Œ!\")\n",
    "print(f\"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(f\"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ”¢ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•©ì„± íƒœì•„ ì´ˆìŒíŒŒ ë°ì´í„° ìƒì„±ê¸°\n",
    "class FetalDataGenerator:\n",
    "    \"\"\"íƒœì•„ ì„±ë³„ ì˜ˆì¸¡ì„ ìœ„í•œ í•©ì„± ë°ì´í„° ìƒì„±ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=1000, image_size=(224, 224), random_seed=42):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def generate_clinical_data(self):\n",
    "        \"\"\"ì„ìƒ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ìˆ˜ì¹˜í˜• íŠ¹ì§•)\"\"\"\n",
    "        \n",
    "        # ì„±ë³„ ë¼ë²¨ ìƒì„± (0: ì—¬ì„±, 1: ë‚¨ì„±)\n",
    "        gender_labels = np.random.randint(0, 2, self.num_samples)\n",
    "        \n",
    "        clinical_features = []\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            gender = gender_labels[i]\n",
    "            \n",
    "            # ì„±ë³„ì— ë”°ë¥¸ íŠ¹ì§• ë¶„í¬ ì„¤ì •\n",
    "            if gender == 1:  # ë‚¨ì„±\n",
    "                gestational_age = np.random.normal(28, 4)  # ì£¼ìˆ˜\n",
    "                head_circumference = np.random.normal(265, 15)  # ë¨¸ë¦¬ë‘˜ë ˆ(mm)\n",
    "                femur_length = np.random.normal(52, 8)  # ëŒ€í‡´ê³¨ ê¸¸ì´(mm)\n",
    "                estimated_weight = np.random.normal(1800, 300)  # ì˜ˆìƒ ì²´ì¤‘(g)\n",
    "                heart_rate = np.random.normal(145, 10)  # ì‹¬ë°•ìˆ˜\n",
    "                biparietal_diameter = np.random.normal(72, 5)  # ì–‘ë‘ì •ê°„ê²½(mm)\n",
    "            else:  # ì—¬ì„±\n",
    "                gestational_age = np.random.normal(28, 4)\n",
    "                head_circumference = np.random.normal(260, 15)\n",
    "                femur_length = np.random.normal(50, 8)\n",
    "                estimated_weight = np.random.normal(1750, 300)\n",
    "                heart_rate = np.random.normal(150, 10)\n",
    "                biparietal_diameter = np.random.normal(70, 5)\n",
    "            \n",
    "            # ê°’ ë²”ìœ„ ì œí•œ\n",
    "            gestational_age = np.clip(gestational_age, 20, 40)\n",
    "            head_circumference = np.clip(head_circumference, 200, 350)\n",
    "            femur_length = np.clip(femur_length, 30, 80)\n",
    "            estimated_weight = np.clip(estimated_weight, 800, 3500)\n",
    "            heart_rate = np.clip(heart_rate, 120, 180)\n",
    "            biparietal_diameter = np.clip(biparietal_diameter, 50, 100)\n",
    "            \n",
    "            clinical_features.append([\n",
    "                gestational_age,\n",
    "                head_circumference,\n",
    "                femur_length,\n",
    "                estimated_weight,\n",
    "                heart_rate,\n",
    "                biparietal_diameter\n",
    "            ])\n",
    "        \n",
    "        feature_names = [\n",
    "            'gestational_age',  # ì£¼ìˆ˜\n",
    "            'head_circumference',  # ë¨¸ë¦¬ë‘˜ë ˆ\n",
    "            'femur_length',  # ëŒ€í‡´ê³¨ ê¸¸ì´\n",
    "            'estimated_weight',  # ì˜ˆìƒ ì²´ì¤‘\n",
    "            'heart_rate',  # ì‹¬ë°•ìˆ˜\n",
    "            'biparietal_diameter'  # ì–‘ë‘ì •ê°„ê²½\n",
    "        ]\n",
    "        \n",
    "        return np.array(clinical_features), gender_labels, feature_names\n",
    "    \n",
    "    def generate_ultrasound_images(self, gender_labels):\n",
    "        \"\"\"ì´ˆìŒíŒŒ ì´ë¯¸ì§€ ë°ì´í„° ìƒì„± (í•©ì„±)\"\"\"\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            gender = gender_labels[i]\n",
    "            \n",
    "            # ê¸°ë³¸ ë°°ê²½ ìƒì„±\n",
    "            image = np.random.rand(self.image_size[0], self.image_size[1], 3) * 50\n",
    "            \n",
    "            # íƒœì•„ ì˜ì—­ ì‹œë®¬ë ˆì´ì…˜\n",
    "            center_x, center_y = self.image_size[0]//2, self.image_size[1]//2\n",
    "            \n",
    "            # ì„±ë³„ì— ë”°ë¥¸ íŠ¹ì§•ì  íŒ¨í„´ ì¶”ê°€\n",
    "            if gender == 1:  # ë‚¨ì„±\n",
    "                # ë‚¨ì„±ì  íŠ¹ì§• íŒ¨í„´ (ë” ì„ ëª…í•œ êµ¬ì¡°ë¬¼)\n",
    "                cv2.circle(image, (center_x, center_y), 40, (120, 120, 120), -1)\n",
    "                cv2.rectangle(image, (center_x-15, center_y+20), (center_x+15, center_y+50), (80, 80, 80), -1)\n",
    "            else:  # ì—¬ì„±\n",
    "                # ì—¬ì„±ì  íŠ¹ì§• íŒ¨í„´ (ë” ë¶€ë“œëŸ¬ìš´ êµ¬ì¡°ë¬¼)\n",
    "                cv2.circle(image, (center_x, center_y), 35, (100, 100, 100), -1)\n",
    "                cv2.ellipse(image, (center_x, center_y+30), (25, 15), 0, 0, 360, (90, 90, 90), -1)\n",
    "            \n",
    "            # ë…¸ì´ì¦ˆ ì¶”ê°€ (ì´ˆìŒíŒŒ íŠ¹ì„±)\n",
    "            noise = np.random.normal(0, 20, image.shape)\n",
    "            image = np.clip(image + noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # ì´ˆìŒíŒŒ íŠ¹ì„± í•„í„° ì ìš©\n",
    "            image = cv2.GaussianBlur(image, (5, 5), 1)\n",
    "            \n",
    "            images.append(image)\n",
    "        \n",
    "        return np.array(images)\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        \"\"\"ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "        \n",
    "        print(f\"ğŸ“Š {self.num_samples}ê°œ ìƒ˜í”Œ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ìƒì„±\n",
    "        clinical_data, gender_labels, feature_names = self.generate_clinical_data()\n",
    "        \n",
    "        # ì´ˆìŒíŒŒ ì´ë¯¸ì§€ ìƒì„±  \n",
    "        ultrasound_images = self.generate_ultrasound_images(gender_labels)\n",
    "        \n",
    "        dataset = {\n",
    "            'clinical_data': clinical_data,\n",
    "            'ultrasound_images': ultrasound_images,\n",
    "            'gender_labels': gender_labels,\n",
    "            'feature_names': feature_names,\n",
    "            'class_names': ['Female', 'Male']\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ!\")\n",
    "        print(f\"   - ì„ìƒ ë°ì´í„° í˜•íƒœ: {clinical_data.shape}\")\n",
    "        print(f\"   - ì´ë¯¸ì§€ ë°ì´í„° í˜•íƒœ: {ultrasound_images.shape}\")\n",
    "        print(f\"   - ì„±ë³„ ë¶„í¬: ì—¬ì„± {(gender_labels==0).sum()}ëª…, ë‚¨ì„± {(gender_labels==1).sum()}ëª…\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "print(\"ğŸ”„ í•©ì„± íƒœì•„ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
    "data_generator = FetalDataGenerator(num_samples=2000, random_seed=42)\n",
    "dataset = data_generator.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°í™”\n",
    "\n",
    "def preprocess_and_visualize_data(dataset):\n",
    "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°í™”\"\"\"\n",
    "    \n",
    "    # ë°ì´í„° ì¶”ì¶œ\n",
    "    clinical_data = dataset['clinical_data']\n",
    "    images = dataset['ultrasound_images']\n",
    "    labels = dataset['gender_labels']\n",
    "    feature_names = dataset['feature_names']\n",
    "    class_names = dataset['class_names']\n",
    "    \n",
    "    # 1. ì„ìƒ ë°ì´í„° í‘œì¤€í™”\n",
    "    scaler = StandardScaler()\n",
    "    clinical_data_scaled = scaler.fit_transform(clinical_data)\n",
    "    \n",
    "    # 2. ë°ì´í„° ë¶„í•  (train/val/test)\n",
    "    # ë¨¼ì € train+valê³¼ testë¡œ ë¶„í• \n",
    "    X_clinical_temp, X_clinical_test, X_images_temp, X_images_test, y_temp, y_test = train_test_split(\n",
    "        clinical_data_scaled, images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # train+valì„ trainê³¼ valë¡œ ë¶„í• \n",
    "    X_clinical_train, X_clinical_val, X_images_train, X_images_val, y_train, y_val = train_test_split(\n",
    "        X_clinical_temp, X_images_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í¬ ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ“Š ë°ì´í„° ë¶„í¬ ë° íŠ¹ì„± ë¶„ì„', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. ì„±ë³„ ë¶„í¬\n",
    "    axes[0, 0].bar(class_names, [np.sum(labels == 0), np.sum(labels == 1)], \n",
    "                   color=['pink', 'lightblue'], alpha=0.8)\n",
    "    axes[0, 0].set_title('ì„±ë³„ ë¶„í¬', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('ìƒ˜í”Œ ìˆ˜')\n",
    "    \n",
    "    # 2. ì„ìƒ íŠ¹ì§• ë¶„í¬ (ì„±ë³„ë³„)\n",
    "    clinical_df = pd.DataFrame(clinical_data, columns=feature_names)\n",
    "    clinical_df['gender'] = ['Female' if l == 0 else 'Male' for l in labels]\n",
    "    \n",
    "    # ì£¼ìš” íŠ¹ì§• ì„ íƒí•˜ì—¬ ë°•ìŠ¤í”Œë¡¯\n",
    "    key_features = ['gestational_age', 'head_circumference', 'femur_length']\n",
    "    for i, feature in enumerate(key_features):\n",
    "        sns.boxplot(data=clinical_df, x='gender', y=feature, ax=axes[0, i+1])\n",
    "        axes[0, i+1].set_title(f'{feature}', fontweight='bold')\n",
    "    \n",
    "    # 3. ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "    sample_indices = np.random.choice(len(images), 6, replace=False)\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        row, col = i // 3, i % 3\n",
    "        axes[1, col].imshow(images[idx])\n",
    "        axes[1, col].set_title(f'{class_names[labels[idx]]} (Sample {idx})', fontweight='bold')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = pd.DataFrame(clinical_data, columns=feature_names).corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, cbar_kws={'label': 'Correlation'})\n",
    "    plt.title('ì„ìƒ íŠ¹ì§• ê°„ ìƒê´€ê´€ê³„', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë°˜í™˜\n",
    "    preprocessed_data = {\n",
    "        'X_clinical_train': X_clinical_train,\n",
    "        'X_clinical_val': X_clinical_val, \n",
    "        'X_clinical_test': X_clinical_test,\n",
    "        'X_images_train': X_images_train,\n",
    "        'X_images_val': X_images_val,\n",
    "        'X_images_test': X_images_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    print(f\"   ğŸ“ˆ í›ˆë ¨ ì„¸íŠ¸: {len(y_train)} ìƒ˜í”Œ\")\n",
    "    print(f\"   ğŸ” ê²€ì¦ ì„¸íŠ¸: {len(y_val)} ìƒ˜í”Œ\") \n",
    "    print(f\"   ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(y_test)} ìƒ˜í”Œ\")\n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "data = preprocess_and_visualize_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ ì •ì˜\n",
    "\n",
    "class TextOnlyModel(nn.Module):\n",
    "    \"\"\"í…ìŠ¤íŠ¸(ì„ìƒ ë°ì´í„°) ì „ìš© ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], num_classes=2, dropout=0.3):\n",
    "        super(TextOnlyModel, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CNNImageModel(nn.Module):\n",
    "    \"\"\"CNN ê¸°ë°˜ ì´ë¯¸ì§€ ì „ìš© ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNImageModel, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class ViTOnlyModel(nn.Module):\n",
    "    \"\"\"ViT ê¸°ë°˜ ì´ë¯¸ì§€ ì „ìš© ë¶„ë¥˜ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(ViTOnlyModel, self).__init__()\n",
    "        \n",
    "        # ViT ëª¨ë¸ ë¡œë“œ\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # ë¶„ë¥˜ í—¤ë“œ\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vit.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # ViTì˜ ì¼ë¶€ ë ˆì´ì–´ ë™ê²° (ì„ íƒì‚¬í•­)\n",
    "        for param in list(self.vit.parameters())[:-4]:  # ë§ˆì§€ë§‰ 4ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "print(\"ğŸ”¨ ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸\n",
    "text_model = TextOnlyModel(input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "\n",
    "# CNN ì´ë¯¸ì§€ ëª¨ë¸\n",
    "cnn_model = CNNImageModel().to(device)\n",
    "\n",
    "# ViT ì „ìš© ëª¨ë¸\n",
    "vit_model = ViTOnlyModel().to(device)\n",
    "\n",
    "print(f\"âœ… ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   ğŸ“Š í…ìŠ¤íŠ¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in text_model.parameters()):,}\")\n",
    "print(f\"   ğŸ–¼ï¸  CNN ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "print(f\"   ğŸ¤– ViT ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in vit_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì •ì˜\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    \"\"\"ì¡°ê¸° ìœµí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (íŠ¹ì§• ë‹¨ê³„ì—ì„œ ê²°í•©)\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_input_dim, vit_model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        \n",
    "        # ViT ë°±ë³¸\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ì¸ì½”ë”\n",
    "        self.clinical_encoder = nn.Sequential(\n",
    "            nn.Linear(clinical_input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ViT íŠ¹ì§• ë³€í™˜\n",
    "        self.vit_projection = nn.Sequential(\n",
    "            nn.Linear(self.vit.config.hidden_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ìœµí•©ëœ íŠ¹ì§• ë¶„ë¥˜ê¸°\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 512, 256),  # ViT + ì„ìƒ ë°ì´í„°\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values, clinical_data):\n",
    "        # ViT íŠ¹ì§• ì¶”ì¶œ\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        vit_features = self.vit_projection(vit_outputs.pooler_output)\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ì¸ì½”ë”©\n",
    "        clinical_features = self.clinical_encoder(clinical_data)\n",
    "        \n",
    "        # íŠ¹ì§• ìœµí•©\n",
    "        fused_features = torch.cat([vit_features, clinical_features], dim=1)\n",
    "        \n",
    "        # ë¶„ë¥˜\n",
    "        logits = self.fusion_classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"í›„ê¸° ìœµí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (ì˜ˆì¸¡ ë‹¨ê³„ì—ì„œ ê²°í•©)\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_input_dim, vit_model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        \n",
    "        # ViT ë¶„ë¥˜ê¸°\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        self.vit_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vit.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ë¶„ë¥˜ê¸°\n",
    "        self.clinical_classifier = nn.Sequential(\n",
    "            nn.Linear(clinical_input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # ìœµí•© ê°€ì¤‘ì¹˜\n",
    "        self.fusion_weights = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        \n",
    "    def forward(self, pixel_values, clinical_data):\n",
    "        # ViT ì˜ˆì¸¡\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        vit_logits = self.vit_classifier(vit_outputs.pooler_output)\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ì˜ˆì¸¡\n",
    "        clinical_logits = self.clinical_classifier(clinical_data)\n",
    "        \n",
    "        # ê°€ì¤‘ ìœµí•©\n",
    "        weights = torch.softmax(self.fusion_weights, dim=0)\n",
    "        fused_logits = weights[0] * vit_logits + weights[1] * clinical_logits\n",
    "        \n",
    "        return fused_logits\n",
    "\n",
    "class AttentionFusionModel(nn.Module):\n",
    "    \"\"\"ì–´í…ì…˜ ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_input_dim, vit_model_name=\"google/vit-base-patch16-224\", num_classes=2):\n",
    "        super(AttentionFusionModel, self).__init__()\n",
    "        \n",
    "        # ViT ë°±ë³¸\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ì¸ì½”ë”\n",
    "        self.clinical_encoder = nn.Sequential(\n",
    "            nn.Linear(clinical_input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "        \n",
    "        # ViT íŠ¹ì§• ë³€í™˜\n",
    "        self.vit_projection = nn.Linear(self.vit.config.hidden_size, 512)\n",
    "        \n",
    "        # í¬ë¡œìŠ¤ ì–´í…ì…˜\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # ë¶„ë¥˜ê¸°\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values, clinical_data):\n",
    "        # ViT íŠ¹ì§• ì¶”ì¶œ\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        vit_features = self.vit_projection(vit_outputs.pooler_output)  # [batch_size, 512]\n",
    "        \n",
    "        # ì„ìƒ ë°ì´í„° ì¸ì½”ë”©\n",
    "        clinical_features = self.clinical_encoder(clinical_data)  # [batch_size, 512]\n",
    "        \n",
    "        # ì–´í…ì…˜ì„ ìœ„í•œ ì°¨ì› í™•ì¥\n",
    "        vit_features = vit_features.unsqueeze(1)  # [batch_size, 1, 512]\n",
    "        clinical_features = clinical_features.unsqueeze(1)  # [batch_size, 1, 512]\n",
    "        \n",
    "        # í¬ë¡œìŠ¤ ì–´í…ì…˜ ì ìš© (clinicalì„ query, vitë¥¼ key/valueë¡œ ì‚¬ìš©)\n",
    "        attended_features, attention_weights = self.cross_attention(\n",
    "            clinical_features, vit_features, vit_features\n",
    "        )\n",
    "        \n",
    "        # ì°¨ì› ì¶•ì†Œ\n",
    "        attended_features = attended_features.squeeze(1)  # [batch_size, 512]\n",
    "        \n",
    "        # ë¶„ë¥˜\n",
    "        logits = self.classifier(attended_features)\n",
    "        return logits\n",
    "\n",
    "# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "print(\"ğŸ”¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...\")\n",
    "\n",
    "clinical_dim = data['X_clinical_train'].shape[1]\n",
    "\n",
    "# Early Fusion ëª¨ë¸\n",
    "early_fusion_model = EarlyFusionModel(clinical_input_dim=clinical_dim).to(device)\n",
    "\n",
    "# Late Fusion ëª¨ë¸  \n",
    "late_fusion_model = LateFusionModel(clinical_input_dim=clinical_dim).to(device)\n",
    "\n",
    "# Attention Fusion ëª¨ë¸\n",
    "attention_fusion_model = AttentionFusionModel(clinical_input_dim=clinical_dim).to(device)\n",
    "\n",
    "print(f\"âœ… ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   ğŸ”— Early Fusion íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in early_fusion_model.parameters()):,}\")\n",
    "print(f\"   ğŸ”— Late Fusion íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in late_fusion_model.parameters()):,}\")\n",
    "print(f\"   ğŸ¯ Attention Fusion íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in attention_fusion_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28612e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ë°ì´í„°ë¡œë” ë° í›ˆë ¨ í•¨ìˆ˜\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, clinical_data, images, labels, processor=None):\n",
    "        self.clinical_data = torch.FloatTensor(clinical_data)\n",
    "        self.images = images\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        clinical = self.clinical_data[idx]\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "        if self.processor:\n",
    "            # ViT í”„ë¡œì„¸ì„œ ì‚¬ìš©\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        else:\n",
    "            # CNNì„ ìœ„í•œ ì¼ë°˜ ì „ì²˜ë¦¬\n",
    "            image = torch.FloatTensor(image).permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        return {\n",
    "            'clinical': clinical,\n",
    "            'image': image, \n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "def create_dataloaders(data, batch_size=32, use_vit_processor=True):\n",
    "    \"\"\"ë°ì´í„°ë¡œë” ìƒì„±\"\"\"\n",
    "    \n",
    "    processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\") if use_vit_processor else None\n",
    "    \n",
    "    train_dataset = MultimodalDataset(\n",
    "        data['X_clinical_train'], data['X_images_train'], data['y_train'], processor\n",
    "    )\n",
    "    val_dataset = MultimodalDataset(\n",
    "        data['X_clinical_val'], data['X_images_val'], data['y_val'], processor\n",
    "    )\n",
    "    test_dataset = MultimodalDataset(\n",
    "        data['X_clinical_test'], data['X_images_test'], data['y_test'], processor\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, model_name=\"model\"):\n",
    "    \"\"\"ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ {model_name} í›ˆë ¨ ì‹œì‘...\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # í›ˆë ¨ ëª¨ë“œ\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            clinical = batch['clinical'].to(device)\n",
    "            image = batch['image'].to(device) \n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ forward pass\n",
    "            if isinstance(model, TextOnlyModel):\n",
    "                outputs = model(clinical)\n",
    "            elif isinstance(model, (CNNImageModel, ViTOnlyModel)):\n",
    "                outputs = model(image)\n",
    "            else:  # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸\n",
    "                outputs = model(image, clinical)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # ê²€ì¦ ëª¨ë“œ\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                clinical = batch['clinical'].to(device)\n",
    "                image = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                if isinstance(model, TextOnlyModel):\n",
    "                    outputs = model(clinical)\n",
    "                elif isinstance(model, (CNNImageModel, ViTOnlyModel)):\n",
    "                    outputs = model(image)\n",
    "                else:\n",
    "                    outputs = model(image, clinical)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # í•™ìŠµë¥  ì—…ë°ì´íŠ¸\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'models/{model_name}_best.pth')\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"   Epoch {epoch+1}/{num_epochs}: \"\n",
    "                  f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, \"\n",
    "                  f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    print(f\"âœ… {model_name} í›ˆë ¨ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# ë°ì´í„°ë¡œë” ìƒì„± (ViTìš©)\n",
    "train_loader_vit, val_loader_vit, test_loader_vit = create_dataloaders(data, batch_size=16, use_vit_processor=True)\n",
    "\n",
    "# ë°ì´í„°ë¡œë” ìƒì„± (CNNìš©) \n",
    "train_loader_cnn, val_loader_cnn, test_loader_cnn = create_dataloaders(data, batch_size=32, use_vit_processor=False)\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd57b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
    "\n",
    "# í›ˆë ¨ ì„¤ì •\n",
    "num_epochs = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "print(\"ğŸ”¥ ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\\\nğŸ“Š í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ í›ˆë ¨\")\n",
    "text_results = train_model(\n",
    "    text_model, \n",
    "    train_loader_vit,  # ì„ìƒ ë°ì´í„°ëŠ” ë™ì¼í•˜ë¯€ë¡œ ì•„ë¬´ê±°ë‚˜ ì‚¬ìš© ê°€ëŠ¥\n",
    "    val_loader_vit, \n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    model_name=\"text_only\"\n",
    ")\n",
    "training_results['text_only'] = text_results\n",
    "\n",
    "# 2. CNN ì´ë¯¸ì§€ ëª¨ë¸ í›ˆë ¨  \n",
    "print(\"\\\\nğŸ–¼ï¸ CNN ì´ë¯¸ì§€ ëª¨ë¸ í›ˆë ¨\")\n",
    "cnn_results = train_model(\n",
    "    cnn_model,\n",
    "    train_loader_cnn,\n",
    "    val_loader_cnn,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    model_name=\"cnn_image\"\n",
    ")\n",
    "training_results['cnn_image'] = cnn_results\n",
    "\n",
    "# 3. ViT ì „ìš© ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\\\nğŸ¤– ViT ì „ìš© ëª¨ë¸ í›ˆë ¨\") \n",
    "vit_results = train_model(\n",
    "    vit_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,  # ViTëŠ” ë” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©\n",
    "    model_name=\"vit_only\"\n",
    ")\n",
    "training_results['vit_only'] = vit_results\n",
    "\n",
    "# 4. Early Fusion ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\\\nğŸ”— Early Fusion ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨\")\n",
    "early_fusion_results = train_model(\n",
    "    early_fusion_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,\n",
    "    model_name=\"early_fusion\"\n",
    ")\n",
    "training_results['early_fusion'] = early_fusion_results\n",
    "\n",
    "# 5. Late Fusion ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\\\nğŸ”— Late Fusion ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨\")\n",
    "late_fusion_results = train_model(\n",
    "    late_fusion_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,\n",
    "    model_name=\"late_fusion\"\n",
    ")\n",
    "training_results['late_fusion'] = late_fusion_results\n",
    "\n",
    "# 6. Attention Fusion ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\\\nğŸ¯ Attention Fusion ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨\")\n",
    "attention_fusion_results = train_model(\n",
    "    attention_fusion_model,\n",
    "    train_loader_vit,\n",
    "    val_loader_vit,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate*0.5,\n",
    "    model_name=\"attention_fusion\"\n",
    ")\n",
    "training_results['attention_fusion'] = attention_fusion_results\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# í›ˆë ¨ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\\\nğŸ“‹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½:\")\n",
    "print(\"-\"*50)\n",
    "for model_name, results in training_results.items():\n",
    "    print(f\"{model_name:20}: ìµœê³  ê²€ì¦ ì •í™•ë„ {results['best_val_acc']:.2f}%\")\n",
    "\n",
    "# í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "with open('models/training_results.json', 'w') as f:\n",
    "    # numpy arrayë¥¼ listë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦\n",
    "    json_results = {}\n",
    "    for model_name, results in training_results.items():\n",
    "        json_results[model_name] = {\n",
    "            'train_losses': results['train_losses'],\n",
    "            'val_losses': results['val_losses'],\n",
    "            'val_accuracies': results['val_accuracies'],\n",
    "            'best_val_acc': results['best_val_acc']\n",
    "        }\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(\"\\\\nğŸ’¾ í›ˆë ¨ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58089c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ëª¨ë¸ í‰ê°€ ë° ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "def evaluate_model(model, test_loader, model_name):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            clinical = batch['clinical'].to(device)\n",
    "            image = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ forward pass\n",
    "            if isinstance(model, TextOnlyModel):\n",
    "                outputs = model(clinical)\n",
    "            elif isinstance(model, (CNNImageModel, ViTOnlyModel)):\n",
    "                outputs = model(image)\n",
    "            else:  # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸\n",
    "                outputs = model(image, clinical)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    # ROC AUC (ì´ì§„ ë¶„ë¥˜)\n",
    "    probabilities_positive = [prob[1] for prob in all_probabilities]  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ \n",
    "    roc_auc = roc_auc_score(all_labels, probabilities_positive)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ë° í‰ê°€\n",
    "models = {\n",
    "    'text_only': text_model,\n",
    "    'cnn_image': cnn_model, \n",
    "    'vit_only': vit_model,\n",
    "    'early_fusion': early_fusion_model,\n",
    "    'late_fusion': late_fusion_model,\n",
    "    'attention_fusion': attention_fusion_model\n",
    "}\n",
    "\n",
    "# ìµœì  ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f'models/{model_name}_best.pth'))\n",
    "        print(f\"âœ… {model_name} ìµœì  ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ {model_name} ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\\\nğŸ§ª ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸ í‰ê°€ ì‹œì‘...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ëª¨ë¸ í‰ê°€ (ViT ë¡œë” ì‚¬ìš©, ì„ìƒ ë°ì´í„°ë§Œ í•„ìš”)\n",
    "evaluation_results['text_only'] = evaluate_model(text_model, test_loader_vit, 'Text Only')\n",
    "\n",
    "# CNN ëª¨ë¸ í‰ê°€\n",
    "evaluation_results['cnn_image'] = evaluate_model(cnn_model, test_loader_cnn, 'CNN Image')\n",
    "\n",
    "# ViT ëª¨ë¸ í‰ê°€  \n",
    "evaluation_results['vit_only'] = evaluate_model(vit_model, test_loader_vit, 'ViT Only')\n",
    "\n",
    "# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë“¤ í‰ê°€\n",
    "evaluation_results['early_fusion'] = evaluate_model(early_fusion_model, test_loader_vit, 'Early Fusion')\n",
    "evaluation_results['late_fusion'] = evaluate_model(late_fusion_model, test_loader_vit, 'Late Fusion') \n",
    "evaluation_results['attention_fusion'] = evaluate_model(attention_fusion_model, test_loader_vit, 'Attention Fusion')\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")\n",
    "\n",
    "# ê²°ê³¼ í…Œì´ë¸” ìƒì„±\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Accuracy': f\"{result['accuracy']:.4f}\",\n",
    "        'Precision': f\"{result['precision']:.4f}\",\n",
    "        'Recall': f\"{result['recall']:.4f}\",\n",
    "        'F1-Score': f\"{result['f1_score']:.4f}\",\n",
    "        'ROC-AUC': f\"{result['roc_auc']:.4f}\"\n",
    "    }\n",
    "    for result in evaluation_results.values()\n",
    "])\n",
    "\n",
    "print(\"\\\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„\n",
    "\n",
    "def create_performance_visualization(training_results, evaluation_results):\n",
    "    \"\"\"ì„±ëŠ¥ ì‹œê°í™” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ì„œë¸Œí”Œë¡¯ ìƒì„±\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('ğŸ¯ ë©€í‹°ëª¨ë‹¬ íƒœì•„ ì„±ë³„ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. í›ˆë ¨ ì†ì‹¤ ë¹„êµ\n",
    "    axes[0, 0].set_title('ğŸ“‰ Training Loss Curves', fontweight='bold', fontsize=14)\n",
    "    for model_name, results in training_results.items():\n",
    "        axes[0, 0].plot(results['train_losses'], label=model_name, linewidth=2, alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Training Loss')\n",
    "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ê²€ì¦ ì •í™•ë„ ë¹„êµ\n",
    "    axes[0, 1].set_title('ğŸ“ˆ Validation Accuracy Curves', fontweight='bold', fontsize=14)\n",
    "    for model_name, results in training_results.items():\n",
    "        axes[0, 1].plot(results['val_accuracies'], label=model_name, linewidth=2, alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy (%)')\n",
    "    axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë°” ì°¨íŠ¸\n",
    "    model_names = [result['model_name'] for result in evaluation_results.values()]\n",
    "    accuracies = [result['accuracy'] for result in evaluation_results.values()]\n",
    "    f1_scores = [result['f1_score'] for result in evaluation_results.values()]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 2].bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "    axes[0, 2].bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    axes[0, 2].set_title('ğŸ¯ Test Performance Comparison', fontweight='bold', fontsize=14)\n",
    "    axes[0, 2].set_ylabel('Score')\n",
    "    axes[0, 2].set_xticks(x)\n",
    "    axes[0, 2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ê°’ í‘œì‹œ\n",
    "    for i, (acc, f1) in enumerate(zip(accuracies, f1_scores)):\n",
    "        axes[0, 2].text(i - width/2, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        axes[0, 2].text(i + width/2, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. ROC AUC ë¹„êµ\n",
    "    roc_aucs = [result['roc_auc'] for result in evaluation_results.values()]\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "    \n",
    "    bars = axes[1, 0].bar(model_names, roc_aucs, color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_title('ğŸ“Š ROC-AUC Comparison', fontweight='bold', fontsize=14)\n",
    "    axes[1, 0].set_ylabel('ROC-AUC Score')\n",
    "    axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ê°’ í‘œì‹œ\n",
    "    for bar, auc in zip(bars, roc_aucs):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                       f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. í˜¼ë™í–‰ë ¬ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)\n",
    "    best_model_name = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['accuracy'])\n",
    "    best_result = evaluation_results[best_model_name]\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(best_result['labels'], best_result['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n",
    "                xticklabels=data['class_names'], yticklabels=data['class_names'])\n",
    "    axes[1, 1].set_title(f'ğŸ¯ Confusion Matrix\\\\n({best_result[\"model_name\"]})', fontweight='bold', fontsize=14)\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('Actual')\n",
    "    \n",
    "    # 6. ëª¨ë¸ ë³µì¡ë„ vs ì„±ëŠ¥\n",
    "    model_params = {\n",
    "        'Text Only': sum(p.numel() for p in text_model.parameters()),\n",
    "        'CNN Image': sum(p.numel() for p in cnn_model.parameters()),\n",
    "        'ViT Only': sum(p.numel() for p in vit_model.parameters()),\n",
    "        'Early Fusion': sum(p.numel() for p in early_fusion_model.parameters()),\n",
    "        'Late Fusion': sum(p.numel() for p in late_fusion_model.parameters()),\n",
    "        'Attention Fusion': sum(p.numel() for p in attention_fusion_model.parameters())\n",
    "    }\n",
    "    \n",
    "    params_list = [model_params[name] for name in model_names]\n",
    "    \n",
    "    scatter = axes[1, 2].scatter(params_list, accuracies, c=colors, s=100, alpha=0.8)\n",
    "    axes[1, 2].set_title('ğŸ”§ Model Complexity vs Performance', fontweight='bold', fontsize=14)\n",
    "    axes[1, 2].set_xlabel('Number of Parameters')\n",
    "    axes[1, 2].set_ylabel('Test Accuracy')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].set_xscale('log')\n",
    "    \n",
    "    # ëª¨ë¸ ì´ë¦„ í‘œì‹œ\n",
    "    for i, (name, params, acc) in enumerate(zip(model_names, params_list, accuracies)):\n",
    "        axes[1, 2].annotate(name, (params, acc), xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=9, alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_analysis(evaluation_results):\n",
    "    \"\"\"ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“‹ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. ë‹¨ì¼ ëª¨ë‹¬ vs ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥ ë¹„êµ\n",
    "    unimodal_models = ['text_only', 'cnn_image', 'vit_only']\n",
    "    multimodal_models = ['early_fusion', 'late_fusion', 'attention_fusion']\n",
    "    \n",
    "    unimodal_avg_acc = np.mean([evaluation_results[model]['accuracy'] for model in unimodal_models])\n",
    "    multimodal_avg_acc = np.mean([evaluation_results[model]['accuracy'] for model in multimodal_models])\n",
    "    \n",
    "    print(f\"\\\\nğŸ” ë‹¨ì¼ ëª¨ë‹¬ vs ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥:\")\n",
    "    print(f\"   ğŸ“Š ë‹¨ì¼ ëª¨ë‹¬ í‰ê·  ì •í™•ë„: {unimodal_avg_acc:.4f}\")\n",
    "    print(f\"   ğŸ”— ë©€í‹°ëª¨ë‹¬ í‰ê·  ì •í™•ë„: {multimodal_avg_acc:.4f}\")\n",
    "    print(f\"   ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: {(multimodal_avg_acc - unimodal_avg_acc)*100:.2f}%p\")\n",
    "    \n",
    "    # 2. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¶„ì„\n",
    "    best_model = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['accuracy'])\n",
    "    best_acc = evaluation_results[best_model]['accuracy']\n",
    "    \n",
    "    print(f\"\\\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:\")\n",
    "    print(f\"   ëª¨ë¸: {evaluation_results[best_model]['model_name']}\")\n",
    "    print(f\"   ì •í™•ë„: {best_acc:.4f}\")\n",
    "    print(f\"   F1-Score: {evaluation_results[best_model]['f1_score']:.4f}\")\n",
    "    print(f\"   ROC-AUC: {evaluation_results[best_model]['roc_auc']:.4f}\")\n",
    "    \n",
    "    # 3. ìœµí•© ì „ëµ ë¹„êµ\n",
    "    print(f\"\\\\nğŸ”— ë©€í‹°ëª¨ë‹¬ ìœµí•© ì „ëµ ë¹„êµ:\")\n",
    "    for model in multimodal_models:\n",
    "        result = evaluation_results[model]\n",
    "        print(f\"   {result['model_name']:15}: Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\")\n",
    "    \n",
    "    # 4. ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„ ë¶„ì„\n",
    "    text_acc = evaluation_results['text_only']['accuracy']\n",
    "    vit_acc = evaluation_results['vit_only']['accuracy']\n",
    "    best_multimodal_acc = max([evaluation_results[model]['accuracy'] for model in multimodal_models])\n",
    "    \n",
    "    print(f\"\\\\nğŸ“Š ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„:\")\n",
    "    print(f\"   í…ìŠ¤íŠ¸ ë‹¨ë…: {text_acc:.4f}\")\n",
    "    print(f\"   ViT ë‹¨ë…: {vit_acc:.4f}\")  \n",
    "    print(f\"   ìµœê³  ë©€í‹°ëª¨ë‹¬: {best_multimodal_acc:.4f}\")\n",
    "    print(f\"   í…ìŠ¤íŠ¸ ëŒ€ë¹„ í–¥ìƒ: {(best_multimodal_acc - text_acc)*100:.2f}%p\")\n",
    "    print(f\"   ViT ëŒ€ë¹„ í–¥ìƒ: {(best_multimodal_acc - vit_acc)*100:.2f}%p\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# ì‹œê°í™” ì‹¤í–‰\n",
    "create_performance_visualization(training_results, evaluation_results)\n",
    "\n",
    "# ìƒì„¸ ë¶„ì„ ì‹¤í–‰\n",
    "create_detailed_analysis(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53847f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ê¸°ëŠ¥\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir=\"models\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def save_complete_model(self, model, model_name, scaler=None, processor=None, metadata=None):\n",
    "        \"\"\"ì™„ì „í•œ ëª¨ë¸ ì •ë³´ë¥¼ ì €ì¥\"\"\"\n",
    "        \n",
    "        model_info = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'model_config': self._get_model_config(model),\n",
    "            'scaler': scaler,\n",
    "            'processor_name': \"google/vit-base-patch16-224\" if processor else None,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_complete.pkl\")\n",
    "        torch.save(model_info, save_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ {model_name} ì™„ì „ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "        \n",
    "    def _get_model_config(self, model):\n",
    "        \"\"\"ëª¨ë¸ ì„¤ì • ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "        config = {\n",
    "            'class_name': model.__class__.__name__\n",
    "        }\n",
    "        \n",
    "        if hasattr(model, 'vit') and hasattr(model.vit, 'config'):\n",
    "            config['vit_model_name'] = \"google/vit-base-patch16-224\"\n",
    "        \n",
    "        return config\n",
    "        \n",
    "    def load_model(self, model_name, device='cpu'):\n",
    "        \"\"\"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_complete.pkl\")\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            print(f\"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {save_path}\")\n",
    "            return None\n",
    "            \n",
    "        model_info = torch.load(save_path, map_location=device)\n",
    "        \n",
    "        print(f\"ğŸ“‚ {model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        print(f\"   í´ë˜ìŠ¤: {model_info['model_class']}\")\n",
    "        \n",
    "        return model_info\n",
    "    \n",
    "    def save_experiment_results(self, training_results, evaluation_results, dataset_info):\n",
    "        \"\"\"ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ì €ì¥\"\"\"\n",
    "        \n",
    "        experiment_data = {\n",
    "            'training_results': training_results,\n",
    "            'evaluation_results': {\n",
    "                k: {\n",
    "                    'model_name': v['model_name'],\n",
    "                    'accuracy': v['accuracy'],\n",
    "                    'precision': v['precision'],\n",
    "                    'recall': v['recall'],\n",
    "                    'f1_score': v['f1_score'],\n",
    "                    'roc_auc': v['roc_auc']\n",
    "                } for k, v in evaluation_results.items()\n",
    "            },\n",
    "            'dataset_info': dataset_info,\n",
    "            'experiment_date': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, \"experiment_results.json\")\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(experiment_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "# ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# ëª¨ë“  ëª¨ë¸ ì €ì¥\n",
    "print(\"ğŸ’¾ ëª¨ë“  í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "models_to_save = {\n",
    "    'text_only': text_model,\n",
    "    'cnn_image': cnn_model,\n",
    "    'vit_only': vit_model,\n",
    "    'early_fusion': early_fusion_model,\n",
    "    'late_fusion': late_fusion_model,\n",
    "    'attention_fusion': attention_fusion_model\n",
    "}\n",
    "\n",
    "for model_name, model in models_to_save.items():\n",
    "    metadata = {\n",
    "        'best_val_acc': training_results[model_name]['best_val_acc'],\n",
    "        'test_accuracy': evaluation_results[model_name]['accuracy'],\n",
    "        'test_f1_score': evaluation_results[model_name]['f1_score'],\n",
    "        'model_type': 'unimodal' if model_name in ['text_only', 'cnn_image', 'vit_only'] else 'multimodal'\n",
    "    }\n",
    "    \n",
    "    model_manager.save_complete_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        scaler=data['scaler'],\n",
    "        processor=ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\") if 'vit' in model_name or model_name in ['early_fusion', 'late_fusion', 'attention_fusion'] else None,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "# ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ì €ì¥\n",
    "dataset_info = {\n",
    "    'num_samples': len(dataset['gender_labels']),\n",
    "    'num_features': len(dataset['feature_names']),\n",
    "    'feature_names': dataset['feature_names'],\n",
    "    'class_names': dataset['class_names'],\n",
    "    'train_samples': len(data['y_train']),\n",
    "    'val_samples': len(data['y_val']),\n",
    "    'test_samples': len(data['y_test'])\n",
    "}\n",
    "\n",
    "model_manager.save_experiment_results(training_results, evaluation_results, dataset_info)\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264744bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì˜ˆì‹œ\n",
    "\n",
    "def create_inference_demo():\n",
    "    \"\"\"ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ë°ëª¨\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ë°ëª¨\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "    best_model_name = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['accuracy'])\n",
    "    print(f\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ ë¡œë“œ\n",
    "    model_info = model_manager.load_model(best_model_name, device)\n",
    "    \n",
    "    if model_info is None:\n",
    "        print(\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\")\n",
    "        return\n",
    "    \n",
    "    print(\"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "    print(f\"   í´ë˜ìŠ¤: {model_info['model_class']}\")\n",
    "    print(f\"   ë©”íƒ€ë°ì´í„°: {model_info['metadata']}\")\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "    if best_model_name == 'text_only':\n",
    "        loaded_model = TextOnlyModel(input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    elif best_model_name == 'cnn_image':\n",
    "        loaded_model = CNNImageModel().to(device)\n",
    "    elif best_model_name == 'vit_only':\n",
    "        loaded_model = ViTOnlyModel().to(device)\n",
    "    elif best_model_name == 'early_fusion':\n",
    "        loaded_model = EarlyFusionModel(clinical_input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    elif best_model_name == 'late_fusion':\n",
    "        loaded_model = LateFusionModel(clinical_input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    elif best_model_name == 'attention_fusion':\n",
    "        loaded_model = AttentionFusionModel(clinical_input_dim=data['X_clinical_train'].shape[1]).to(device)\n",
    "    \n",
    "    loaded_model.load_state_dict(model_info['model_state_dict'])\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë¡œ ì¶”ë¡  ì‹œì—°\n",
    "    print(\"\\\\nğŸ§ª ì¶”ë¡  ì‹œì—°:\")\n",
    "    \n",
    "    # ë¬´ì‘ìœ„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ\n",
    "    sample_idx = np.random.randint(0, len(data['X_clinical_test']))\n",
    "    \n",
    "    sample_clinical = torch.FloatTensor(data['X_clinical_test'][sample_idx:sample_idx+1]).to(device)\n",
    "    sample_image = data['X_images_test'][sample_idx]\n",
    "    true_label = data['y_test'][sample_idx]\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "    if 'vit' in best_model_name or best_model_name in ['early_fusion', 'late_fusion', 'attention_fusion']:\n",
    "        processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        sample_image_pil = Image.fromarray(sample_image)\n",
    "        sample_image_processed = processor(sample_image_pil, return_tensors=\"pt\")['pixel_values'].to(device)\n",
    "    else:\n",
    "        sample_image_processed = torch.FloatTensor(sample_image).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "    \n",
    "    # ì¶”ë¡  ì‹¤í–‰\n",
    "    with torch.no_grad():\n",
    "        if best_model_name == 'text_only':\n",
    "            outputs = loaded_model(sample_clinical)\n",
    "        elif best_model_name == 'cnn_image' or best_model_name == 'vit_only':\n",
    "            outputs = loaded_model(sample_image_processed)\n",
    "        else:  # ë©€í‹°ëª¨ë‹¬\n",
    "            outputs = loaded_model(sample_image_processed, sample_clinical)\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    class_names = data['class_names']\n",
    "    print(f\"   ì‹¤ì œ ì„±ë³„: {class_names[true_label]}\")\n",
    "    print(f\"   ì˜ˆì¸¡ ì„±ë³„: {class_names[predicted_class]}\")\n",
    "    print(f\"   ì˜ˆì¸¡ ì‹ ë¢°ë„: {confidence:.4f}\")\n",
    "    print(f\"   ì˜ˆì¸¡ ì •í™•ë„: {'âœ… ì •í™•' if predicted_class == true_label else 'âŒ í‹€ë¦¼'}\")\n",
    "    \n",
    "    # ì„ìƒ ë°ì´í„° ì •ë³´ ì¶œë ¥\n",
    "    print(\"\\\\nğŸ“Š ì„ìƒ ë°ì´í„°:\")\n",
    "    feature_names = data['feature_names']\n",
    "    original_values = model_info['scaler'].inverse_transform(sample_clinical.cpu().numpy())[0]\n",
    "    \n",
    "    for feature_name, value in zip(feature_names, original_values):\n",
    "        print(f\"   {feature_name}: {value:.2f}\")\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(sample_image)\n",
    "    plt.title(f'ì´ˆìŒíŒŒ ì´ë¯¸ì§€\\\\nì‹¤ì œ: {class_names[true_label]}', fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    prob_values = probabilities[0].cpu().numpy()\n",
    "    colors = ['pink', 'lightblue']\n",
    "    bars = plt.bar(class_names, prob_values, color=colors, alpha=0.8)\n",
    "    plt.title(f'ì˜ˆì¸¡ í™•ë¥ \\\\nì˜ˆì¸¡: {class_names[predicted_class]} ({confidence:.3f})', fontweight='bold')\n",
    "    plt.ylabel('í™•ë¥ ')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # í™•ë¥  ê°’ í‘œì‹œ\n",
    "    for bar, prob in zip(bars, prob_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # ì„ìƒ íŠ¹ì§• ë ˆì´ë” ì°¨íŠ¸ (ì£¼ìš” íŠ¹ì§•ë§Œ)\n",
    "    angles = np.linspace(0, 2*np.pi, len(feature_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # ì›í˜•ìœ¼ë¡œ ë§Œë“¤ê¸°\n",
    "    \n",
    "    # ì •ê·œí™”ëœ ê°’ ì‚¬ìš©\n",
    "    normalized_values = sample_clinical.cpu().numpy()[0].tolist()\n",
    "    normalized_values += normalized_values[:1]\n",
    "    \n",
    "    plt.polar(angles, normalized_values, 'o-', linewidth=2, alpha=0.8)\n",
    "    plt.fill(angles, normalized_values, alpha=0.25)\n",
    "    plt.xticks(angles[:-1], [name.replace('_', '\\\\n') for name in feature_names], fontsize=8)\n",
    "    plt.title('ì„ìƒ íŠ¹ì§• í”„ë¡œí•„', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/inference_demo.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# ì¶”ë¡  ë°ëª¨ ì‹¤í–‰\n",
    "demo_model = create_inference_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9525e",
   "metadata": {},
   "source": [
    "## 9. ê²°ë¡  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥\n",
    "\n",
    "### ğŸ¯ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n",
    "\n",
    "ë³¸ ì‹¤í—˜ì—ì„œëŠ” íƒœì•„ ì„±ë³„ ì˜ˆì¸¡ì„ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "#### ğŸ“Š ì£¼ìš” ë°œê²¬ì‚¬í•­:\n",
    "\n",
    "1. **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ìš°ìˆ˜ì„±**: ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ ëŒ€ë¹„ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ ì¼ë°˜ì ìœ¼ë¡œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. **ìœµí•© ì „ëµë³„ ì„±ëŠ¥ ì°¨ì´**: \n",
    "   - Attention Fusion: ê°€ì¥ ì •êµí•œ íŠ¹ì§• ìœµí•©\n",
    "   - Early Fusion: íŠ¹ì§• ë‹¨ê³„ì—ì„œì˜ íš¨ê³¼ì ì¸ ê²°í•©\n",
    "   - Late Fusion: ì˜ˆì¸¡ ë‹¨ê³„ì—ì„œì˜ ì•ˆì •ì ì¸ ê²°í•©\n",
    "\n",
    "3. **ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„**:\n",
    "   - ViT ê¸°ë°˜ ì´ë¯¸ì§€ ëª¨ë¸: ì‹œê°ì  íŠ¹ì§• í•™ìŠµ ìš°ìˆ˜\n",
    "   - í…ìŠ¤íŠ¸ ëª¨ë¸: ì„ìƒ ìˆ˜ì¹˜ ë°ì´í„°ì˜ íŒ¨í„´ í•™ìŠµ\n",
    "   - ìœµí•© ëª¨ë¸: ë‘ ëª¨ë‹¬ë¦¬í‹°ì˜ ìƒí˜¸ ë³´ì™„ì  íš¨ê³¼\n",
    "\n",
    "### ğŸ”¬ ë…¼ë¬¸ê³„íšì„œ ëŒ€ë¹„ ë‹¬ì„± ì‚¬í•­:\n",
    "\n",
    "- âœ… ë‹¨ì¼ ëª¨ë‹¬ ê¸°ë°˜ ëª¨ë¸ (í…ìŠ¤íŠ¸, CNN, ViT) êµ¬í˜„ ë° í‰ê°€\n",
    "- âœ… ë©€í‹°ëª¨ë‹¬ ìœµí•© ì „ëµ (Early, Late, Attention) ë¹„êµ\n",
    "- âœ… ëª¨ë¸ë³„ ì„±ëŠ¥ í‰ê°€ ë° í†µê³„ì  ë¶„ì„\n",
    "- âœ… ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ì„ í†µí•œ ì¬ì‚¬ìš© ê°€ëŠ¥ì„± í™•ë³´\n",
    "- âœ… ì‹œê°í™”ë¥¼ í†µí•œ ê²°ê³¼ ë¶„ì„ ë° í•´ì„\n",
    "\n",
    "### ğŸš€ í–¥í›„ ì—°êµ¬ ë°©í–¥:\n",
    "\n",
    "1. **ì‹¤ì œ ì˜ë£Œ ë°ì´í„° ì ìš©**: AI-Hub íƒœì•„ ì´ˆìŒíŒŒ ë°ì´í„°ì…‹ í™œìš©\n",
    "2. **ëª¨ë¸ ì„±ëŠ¥ ê°œì„ **: \n",
    "   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "   - ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©\n",
    "   - ì•™ìƒë¸” ê¸°ë²• ë„ì…\n",
    "\n",
    "3. **ì„¤ëª… ê°€ëŠ¥í•œ AI ì ìš©**:\n",
    "   - Grad-CAMì„ í†µí•œ ì£¼ìš” ì˜ì—­ ì‹œê°í™”\n",
    "   - SHAP ê°’ì„ í†µí•œ ì„ìƒ íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„\n",
    "\n",
    "4. **ì„ìƒì  ìœ ìš©ì„± ê²€ì¦**:\n",
    "   - ì˜ë£Œì§„ê³¼ì˜ í˜‘ì—…ì„ í†µí•œ ëª¨ë¸ ê²€ì¦\n",
    "   - ì‹¤ì œ ì„ìƒ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "### ğŸ’¡ ê¸°ìˆ ì  ê°œì„  ì‚¬í•­:\n",
    "\n",
    "- ë” í° ê·œëª¨ì˜ ë°ì´í„°ì…‹ í™œìš©\n",
    "- íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸ë§ (ë™ì˜ìƒ ë°ì´í„°)\n",
    "- ê°ì²´ íƒì§€ ëª¨ë¸(YOLO, R-CNN)ì„ í†µí•œ ê´€ì‹¬ ì˜ì—­ ì¶”ì¶œ\n",
    "- 3D CNNì„ í™œìš©í•œ ì‹œê³µê°„ íŠ¹ì§• í•™ìŠµ\n",
    "\n",
    "### ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ ë°©ì•ˆ:\n",
    "\n",
    "1. **ë°ì´í„° í’ˆì§ˆ ê°œì„ **: ê³ í•´ìƒë„ ì´ë¯¸ì§€, ì •ì œëœ ì„ìƒ ë°ì´í„°\n",
    "2. **ëª¨ë¸ ì•„í‚¤í…ì²˜ ìµœì í™”**: ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬, íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜\n",
    "3. **ì „ì´í•™ìŠµ í™œìš©**: ëŒ€ê·œëª¨ ì˜ë£Œ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í™œìš©\n",
    "4. **ë„ë©”ì¸ íŠ¹í™” íŠ¹ì§•**: ì´ˆìŒíŒŒ ì˜ìƒ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ\n",
    "\n",
    "ì´ ì‹¤í—˜ì€ ë©€í‹°ëª¨ë‹¬ ì˜ë£Œ AI ì‹œìŠ¤í…œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œ ì„ìƒ í™˜ê²½ì—ì„œì˜ ì ìš©ì„ ìœ„í•œ ê¸°ë°˜ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
